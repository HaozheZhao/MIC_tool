{
    "image-classification": {
        "definition": "",
        "original_definition": [
            "Image Classification is a fundamental task that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific label. Typically, Image Classification refers to images in which only one object appears and is analyzed. In contrast, object detection involves both classification and localization tasks, and is used to analyze more realistic cases in which multiple objects may exist in an image.",
            "Source: Metamorphic Testing for Object Detection Systems"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1912.12162"
        ],
        "subtasks": [
            "Knowledge Distillation",
            "Few-Shot Image Classification",
            "OOD Detection",
            "Fine-Grained Image Classification"
        ]
    },
    "semantic-segmentation": {
        "definition": "",
        "original_definition": [
            "Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category. Some example benchmarks for this task are Cityscapes, PASCAL VOC and ADE20K. Models are usually evaluated with the Mean Intersection-Over-Union (Mean IoU) and Pixel Accuracy metrics.",
            "( Image credit: CSAILVision )"
        ],
        "original_definition_source": [
            "https://github.com/CSAILVision/semantic-segmentation-pytorch"
        ],
        "subtasks": [
            "Tumor Segmentation",
            "Panoptic Segmentation",
            "3D Semantic Segmentation",
            "Weakly-Supervised Semantic Segmentation"
        ]
    },
    "object-detection": {
        "definition": "",
        "original_definition": [
            "Object detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO, SSD and RetinaNet. Two-stage methods prioritize detection accuracy, and example models include Faster R-CNN, Mask R-CNN and Cascade R-CNN.",
            "The most popular benchmark is the MSCOCO dataset. Models are typically evaluated according to a Mean Average Precision metric.",
            "( Image credit: Detectron )"
        ],
        "original_definition_source": [
            "https://github.com/facebookresearch/detectron"
        ],
        "subtasks": [
            "3D Object Detection",
            "RGB Salient Object Detection",
            "Real-Time Object Detection",
            "RGB-D Salient Object Detection"
        ]
    },
    "domain-adaptation": {
        "definition": "",
        "original_definition": [
            "Domain adaptation is the task of adapting models across domains. This is motivated by the challenge where the test and training datasets fall from different data distributions due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions.",
            "Further readings:",
            "( Image credit: Unsupervised Image-to-Image Translation Networks )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1703.00848v6.pdf"
        ],
        "subtasks": [
            "Unsupervised Domain Adaptation",
            "Domain Generalization",
            "Partial Domain Adaptation",
            "Universal Domain Adaptation"
        ]
    },
    "image-generation": {
        "definition": "",
        "original_definition": [
            "Image generation (synthesis) is the task of generating new images from an existing dataset.",
            "In this section, you can find state-of-the-art leaderboards for unconditional generation. For conditional generation, and other types of image generations, refer to the subtasks.",
            "( Image credit: StyleGAN )"
        ],
        "original_definition_source": [
            "https://github.com/NVlabs/stylegan"
        ],
        "subtasks": [
            "Image-to-Image Translation",
            "Image Inpainting",
            "Conditional Image Generation",
            "Face Generation"
        ]
    },
    "data-augmentation": {
        "definition": "",
        "original_definition": [
            "Data augmentation involves techniques used for increasing the amount of data, based on different modifications, to expand the amount of examples in the original dataset. Data augmentation not only helps to grow the dataset but it also increases the diversity of the dataset. When training machine learning models, data augmentation acts as a regularizer and helps to avoid overfitting.",
            "Data augmentation techniques have been found useful in domains like NLP and computer vision. In computer vision, transformations like cropping, flipping, and rotation are used. In NLP, data augmentation techniques can include swapping, deletion, random insertion, among others.",
            "Further readings:",
            "( Image credit: Albumentations )"
        ],
        "original_definition_source": [
            "https://github.com/albumentations-team/albumentations"
        ],
        "subtasks": [
            "Image Augmentation",
            "Text Augmentation",
            "Data Augmentation for Tabular Data"
        ]
    },
    "meta-learning": {
        "definition": "",
        "original_definition": [
            "Meta-learning is a methodology considered with \"learning to learn\" machine learning algorithms.",
            "( Image credit: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1703.03400v3.pdf"
        ],
        "subtasks": [
            "Few-Shot Learning",
            "Sample Probing"
        ]
    },
    "super-resolution": {
        "definition": "",
        "original_definition": [
            "Super resolution is the task of taking an input of a low resolution (LR) and upscaling it to that of a high resolution.",
            "You can find relevant leaderboards in the subtasks below.",
            "( Credit: MemNet )"
        ],
        "original_definition_source": [
            "https://github.com/tyshiwo/MemNet"
        ],
        "subtasks": [
            "Image Super-Resolution",
            "Single Image Super Resolution",
            "Video Super-Resolution",
            "Multi-Frame Super-Resolution"
        ]
    },
    "2d-human-pose-estimation": {
        "definition": "",
        "original_definition": [
            "What is Human Pose Estimation? Human pose estimation is the process of estimating the configuration of the body (pose) from a single, typically monocular, image. Background. Human pose estimation is one of the key problems in computer vision that has been studied for well over 15 years. The reason for its importance is the abundance of applications that can benefit from such a technology. For example, human pose estimation allows for higher-level reasoning in the context of human-computer interaction and activity recognition; it is also one of the basic building blocks for marker-less motion capture (MoCap) technology. MoCap technology is useful for applications ranging from character animation to clinical analysis of gait pathologies."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Pose Estimation",
            "Deblurring",
            "Image Inpainting",
            "Face Swapping"
        ]
    },
    "3d-face-animation": {
        "definition": "",
        "original_definition": [
            "Image: Cudeiro et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Image Generation",
            "Image Super-Resolution"
        ]
    },
    "pose-estimation": {
        "definition": "",
        "original_definition": [
            "Pose Estimation is a general problem in Computer Vision where the goal is to detect the position and orientation of a person or an object. Usually, this is done by predicting the location of specific keypoints like hands, head, elbows, etc. in case of Human Pose Estimation.",
            "A common benchmark for this task is MPII Human Pose",
            "( Image credit: Real-time 2D Multi-Person Pose Estimation on CPU: Lightweight OpenPose )"
        ],
        "original_definition_source": [
            "https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch"
        ],
        "subtasks": [
            "3D Human Pose Estimation",
            "Keypoint Detection",
            "3D Pose Estimation",
            "Multi-Person Pose Estimation"
        ]
    },
    "autonomous-vehicles": {
        "definition": "",
        "original_definition": [
            "Autonomous vehicles is the task of making a vehicle that can guide itself without human conduction.",
            "Many of the state-of-the-art results can be found at more general task pages such as 3D Object Detection and Semantic Segmentation.",
            "( Image credit: GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision )"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2007.13124"
        ],
        "subtasks": [
            "Autonomous Driving",
            "Self-Driving Cars",
            "Simultaneous Localization and Mapping",
            "Autonomous Navigation"
        ]
    },
    "denoising": {
        "definition": "",
        "original_definition": [
            "Denoising is the task of removing noise from an image.",
            "( Image credit: Beyond a Gaussian Denoiser )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1608.03981v1.pdf"
        ],
        "subtasks": [
            "Image Denoising",
            "Color Image Denoising",
            "Sar Image Despeckling",
            "Grayscale Image Denoising"
        ]
    },
    "2d-object-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Text Generation",
            "Image Inpainting",
            "Video Recognition",
            "Open Vocabulary Object Detection"
        ]
    },
    "video": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Object Tracking",
            "Action Classification",
            "Video Understanding",
            "Video Object Segmentation"
        ]
    },
    "contrastive-learning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Knowledge Graph Embedding"
        ]
    },
    "few-shot-learning": {
        "definition": "",
        "original_definition": [
            "Few-Shot Learning is an example of meta-learning, where a learner is trained on several related tasks, during the meta-training phase, so that it can generalize well to unseen (but related) tasks with just few examples, during the meta-testing phase. An effective approach to the Few-Shot Learning problem is to learn a common representation for various tasks and train task specific classifiers on top of this representation.",
            "Source: Penalty Method for Inversion-Free Deep Bilevel Optimization"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1911.03432"
        ],
        "subtasks": [
            "Few-Shot Image Classification",
            "One-Shot Learning",
            "Few-Shot Semantic Segmentation",
            "Cross-Domain Few-Shot"
        ]
    },
    "activity-recognition": {
        "definition": "",
        "original_definition": [
            "Human Activity Recognition is the problem of identifying events performed by humans given a video input. It is formulated as a binary (or multiclass) classification problem of outputting activity class labels. Activity Recognition is an important problem with many societal applications including smart surveillance, video search/retrieval, intelligent robots, and other monitoring systems.",
            "Source: Learning Latent Sub-events in Activity Videos Using Temporal Attention Filters"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1605.08140"
        ],
        "subtasks": [
            "Action Recognition",
            "Human Activity Recognition",
            "Multimodal Activity Recognition",
            "Group Activity Recognition"
        ]
    },
    "3d-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Style Transfer",
            "Image-to-Image Translation",
            "3D Object Classification",
            "Knee Cartilage Defect Assessment"
        ]
    },
    "anomaly-detection": {
        "definition": "",
        "original_definition": [
            "Humans are able to detect heterogeneous or unexpected patterns in a set of homogeneous natural images. This task is known as anomaly or novelty detection and has a large number of applications. Anomaly detection automation would enable constant quality control by avoiding reduced attention span and facilitating human operator work.",
            "Anomaly detection is a binary classification between the normal and the anomalous classes. However, it is not possible to train a model with full supervision for this task because we frequently lack anomalous examples, and, what is more, anomalies can have unexpected patterns.",
            "[Image source]: GAN-based Anomaly Detection in Imbalance Problems"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Unsupervised Anomaly Detection",
            "Anomaly Detection In Surveillance Videos",
            "Abnormal Event Detection In Video",
            "Self-Supervised Anomaly Detection"
        ]
    },
    "self-supervised-learning": {
        "definition": "",
        "original_definition": [
            "Self-Supervised Learning is proposed for utilizing unlabeled data with the success of supervised learning. Producing a dataset with good labels is expensive, while unlabeled data is being generated all the time. The motivation of Self-Supervised Learning is to make use of the large amount of unlabeled data. The main idea of Self-Supervised Learning is to generate the labels from unlabeled data, according to the structure or characteristics of the data itself, and then train on this unsupervised data in a supervised manner. Self-Supervised Learning is wildly used in representation learning to make a model learn the latent features of the data. This technique is often employed in computer vision, video processing and robot control.",
            "Source: Self-supervised Point Set Local Descriptors for Point Cloud Registration",
            "Image source: LeCun"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2003.05199"
        ],
        "subtasks": [
            "Point Cloud Pre-training",
            "Unsupervised Video Clustering"
        ]
    },
    "facial-recognition-and-modelling": {
        "definition": "",
        "original_definition": [
            "Temporal Action Localization aims to detect activities in the video stream and output beginning and end timestamps. It is closely related to Temporal Action Proposal Generation."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Action Recognition",
            "Skeleton Based Action Recognition",
            "Weakly Supervised Action Localization",
            "Weakly-supervised Temporal Action Localization"
        ]
    },
    "depth-estimation": {
        "definition": "",
        "original_definition": [
            "Depth Estimation is the task of measuring the distance of each pixel relative to the camera. Depth is extracted from either monocular (single) or stereo (multiple views of a scene) images. Traditional methods use multi-view geometry to find the relationship between the images. Newer methods can directly estimate depth by minimizing the regression loss, or by learning to generate a novel view from a sequence. The most popular benchmarks are KITTI and NYUv2. Models are typically evaluated according to a RMS metric.",
            "Source: DIODE: A Dense Indoor and Outdoor DEpth Dataset"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1908.00463"
        ],
        "subtasks": [
            "Monocular Depth Estimation",
            "Stereo Depth Estimation",
            "Depth And Camera Motion",
            "3D Depth Estimation"
        ]
    },
    "optical-character-recognition": {
        "definition": "",
        "original_definition": [
            "Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo, license plates in cars...) or from subtitle text superimposed on an image (for example: from a television broadcast)"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Active Learning",
            "Handwriting Recognition",
            "Handwritten Digit Recognition",
            "Irregular Text Recognition"
        ]
    },
    "zero-shot-learning": {
        "definition": "",
        "original_definition": [
            "Zero-shot learning (ZSL) is a model's ability to detect classes never seen during training. The condition is that the classes are not known during supervised learning.",
            "Earlier work in zero-shot learning use attributes in a two-step approach to infer unknown classes. In the computer vision context, more recent advances learn mappings from image feature space to semantic space. Other approaches learn non-linear multimodal embeddings. In the modern NLP context, language models can be evaluated on downstream tasks without fine tuning.",
            "Benchmark datasets for zero-shot learning include aPY, AwA, and CUB, among others.",
            "( Image credit: Prototypical Networks for Few shot Learning in PyTorch )",
            "Further readings:"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Temporal Action Localization",
            "Generalized Zero-Shot Learning",
            "Compositional Zero-Shot Learning",
            "Multi-label zero-shot learning"
        ]
    },
    "instance-segmentation": {
        "definition": "",
        "original_definition": [
            "Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image.",
            "Image Credit: Deep Occlusion-Aware Instance Segmentation with Overlapping BiLayers, CVPR'21"
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Instance Segmentation",
            "Referring Expression Segmentation",
            "Real-time Instance Segmentation",
            "Unsupervised Object Segmentation"
        ]
    },
    "action-recognition": {
        "definition": "",
        "original_definition": [
            "Temporal Action Localization aims to detect activities in the video stream and output beginning and end timestamps. It is closely related to Temporal Action Proposal Generation."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Action Recognition",
            "Skeleton Based Action Recognition",
            "Weakly Supervised Action Localization",
            "Weakly-supervised Temporal Action Localization"
        ]
    },
    "object-tracking": {
        "definition": "",
        "original_definition": [
            "Object tracking is the task of taking an initial set of object detections, creating a unique ID for each of the initial detections, and then tracking each of the objects as they move around frames in a video, maintaining the ID assignment. State-of-the-art methods involve fusing data from RGB and event-based cameras to produce more reliable object tracking. CNN-based models using only RGB images as input are also effective. The most popular benchmark is OTB. There are several evaluation metrics specific to object tracking, including HOTA, MOTA, IDF1, and Track-mAP.",
            "( Image credit: Towards-Realtime-MOT )"
        ],
        "original_definition_source": [
            "https://github.com/Zhongdao/Towards-Realtime-MOT"
        ],
        "subtasks": [
            "Multi-Object Tracking",
            "Visual Object Tracking",
            "Multiple Object Tracking",
            "Online Multi-Object Tracking"
        ]
    },
    "medical-image-segmentation": {
        "definition": "",
        "original_definition": [
            "Medical image segmentation is the task of segmenting objects of interest in a medical image.",
            "( Image credit: IVD-Net )"
        ],
        "original_definition_source": [
            "https://github.com/josedolz/IVD-Net"
        ],
        "subtasks": [
            "Lesion Segmentation",
            "Brain Tumor Segmentation",
            "Brain Segmentation",
            "Cell Segmentation"
        ]
    },
    "knowledge-distillation": {
        "definition": "",
        "original_definition": [
            "Knowledge distillation is the process of transferring knowledge from a large model to a smaller one. While large models (such as very deep neural networks or ensembles of many models) have higher knowledge capacity than small models, this capacity might not be fully utilized."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Self-Knowledge Distillation"
        ]
    },
    "fairness": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Exposure Fairness"
        ]
    },
    "video-classification": {
        "definition": "",
        "original_definition": [
            "Video Classification is the task of producing a label that is relevant to the video given its frames. A good video level classifier is one that not only provides accurate frame labels, but also best describes the entire video given the features and the annotations of the various frames in the video. For example, a video might contain a tree in some frame, but the label that is central to the video might be something else (e.g., \u201chiking\u201d). The granularity of the labels that are needed to describe the frames and the video depends on the task. Typical tasks include assigning one or more global labels to the video, and assigning one or more labels for each frame inside the video.",
            "Source: Efficient Large Scale Video Classification"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1505.06250"
        ],
        "subtasks": [
            "Action Recognition"
        ]
    },
    "quantization": {
        "definition": "",
        "original_definition": [
            "Quantization is a promising technique to reduce the computation cost of neural network training, which can replace high-cost floating-point numbers (e.g., float32) with low-cost fixed-point numbers (e.g., int8/int16).",
            "Source: Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1911.00361"
        ],
        "subtasks": [
            "Data Free Quantization",
            "UNET Quantization"
        ]
    },
    "visual-question-answering": {
        "definition": "",
        "original_definition": [
            "Visual Question Answering is a semantic task that aims to answer questions based on an image.",
            "Image Source: visualqa.org"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Machine Reading Comprehension",
            "Embodied Question Answering",
            "Factual Visual Question Answering"
        ]
    },
    "scene-parsing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Scene Understanding",
            "Scene Text Recognition",
            "Scene Graph Generation",
            "Scene Recognition"
        ]
    },
    "dimensionality-reduction": {
        "definition": "",
        "original_definition": [
            "Dimensionality reduction is the task of reducing the dimensionality of a dataset.",
            "( Image credit: openTSNE )"
        ],
        "original_definition_source": [
            "https://github.com/pavlin-policar/openTSNE"
        ],
        "subtasks": [
            "Supervised dimensionality reduction",
            "Online nonnegative CP decomposition"
        ]
    },
    "image-retrieval": {
        "definition": "",
        "original_definition": [
            "Image retrieval systems aim to find similar images to a query image among an image dataset.",
            "( Image credit: DELF )"
        ],
        "original_definition_source": [
            "https://github.com/tensorflow/models/tree/master/research/delf"
        ],
        "subtasks": [
            "Sketch-Based Image Retrieval",
            "Content-Based Image Retrieval",
            "Text-Image Retrieval",
            "Medical Image Retrieval"
        ]
    },
    "continual-learning": {
        "definition": "",
        "original_definition": [
            "Continual Learning (also known as Incremental Learning, Life-long Learning) is a concept to learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks, where the data in the old tasks are not available anymore during training new ones.\nIf not mentioned, the benchmarks here are Task-CL, where task-id is provided on validation.",
            "Source:\nContinual Learning by Asymmetric Loss Approximation with Single-Side Overestimation\nThree scenarios for continual learning\nLifelong Machine Learning\nContinual lifelong learning with neural networks: A review"
        ],
        "original_definition_source": [],
        "subtasks": [
            "class-incremental learning"
        ]
    },
    "style-transfer": {
        "definition": "",
        "original_definition": [
            "Style transfer is the task of changing the style of an image in one domain to the style of an image in another domain.",
            "( Image credit: A Neural Algorithm of Artistic Style )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1508.06576v2.pdf"
        ],
        "subtasks": [
            "Image Stylization",
            "Style Generalization",
            "Face Transfer",
            "Reverse Style Transfer"
        ]
    },
    "person-re-identification": {
        "definition": "",
        "original_definition": [
            "Person re-identification is the task of associating images of the same person taken from different cameras or from the same camera in different occasions.",
            "( Image credit: PRID2011 dataset )"
        ],
        "original_definition_source": [
            "https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/PRID11/"
        ],
        "subtasks": [
            "Unsupervised Person Re-Identification",
            "Video-Based Person Re-Identification",
            "Generalizable Person Re-identification",
            "Large-Scale Person Re-Identification"
        ]
    },
    "action-localization": {
        "definition": "",
        "original_definition": [
            "Action Localization is finding the spatial and temporal co ordinates for an action in a video. An action localization model will identify which frame an action start and ends in video and return the x,y coordinates of an action. Further the co ordinates will change when the object performing action undergoes a displacement."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Temporal Action Localization",
            "Action Segmentation",
            "Spatio-Temporal Action Localization"
        ]
    },
    "active-learning": {
        "definition": "",
        "original_definition": [
            "Active Learning is a paradigm in supervised machine learning which uses fewer training examples to achieve better optimization by iteratively training a predictor, and using the predictor in each iteration to choose the training examples which will increase its chances of finding better configurations and at the same time improving the accuracy of the prediction model",
            "Source: Polystore++: Accelerated Polystore System for Heterogeneous Workloads"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.10336"
        ],
        "subtasks": [
            "Active Object Detection"
        ]
    },
    "adversarial-attack": {
        "definition": "",
        "original_definition": [
            "An Adversarial Attack is a technique to find a perturbation that changes the prediction of a machine learning model. The perturbation can be very small and imperceptible to human eyes.",
            "Source: Recurrent Attention Model with Log-Polar Mapping is Robust against Adversarial Attacks"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2002.05388"
        ],
        "subtasks": [
            "Backdoor Attack",
            "Adversarial Attack Detection",
            "Real-World Adversarial Attack"
        ]
    },
    "optical-flow-estimation": {
        "definition": "",
        "original_definition": [
            "Optical Flow Estimation is the problem of finding pixel-wise motions between consecutive images.",
            "Approaches for optical flow estimation include correlation-based, block-matching, feature tracking, energy-based, and more recently gradient-based.",
            "Further readings:",
            "Definition source: Devon: Deformable Volume Network for Learning Optical Flow",
            "Image credit: Optical Flow Estimation"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Video Stabilization"
        ]
    },
    "metric-learning": {
        "definition": "",
        "original_definition": [
            "Image: Rahmani et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Deblurring",
            "Skeleton Based Action Recognition",
            "3D Semantic Segmentation",
            "Talking Face Generation"
        ]
    },
    "emotion-recognition": {
        "definition": "",
        "original_definition": [
            "Emotion Recognition is an important area of research to enable effective human-computer interaction. Human emotions can be detected using speech signal, facial expressions, body language, and electroencephalography (EEG). Source: Using Deep Autoencoders for Facial Expression Recognition"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1801.08329"
        ],
        "subtasks": [
            "Speech Emotion Recognition",
            "Emotion Recognition in Conversation",
            "Multimodal Emotion Recognition",
            "Emotion-Cause Pair Extraction"
        ]
    },
    "image-super-resolution": {
        "definition": "",
        "original_definition": [
            "In this task, we try to upsample the image and create a high-resolution image with help of a low-resolution image. The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Multi-Frame Super-Resolution",
            "Burst Image Super-Resolution",
            "Stereo Image Super-Resolution",
            "satellite image super-resolution"
        ]
    },
    "image-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "MRI Reconstruction"
        ]
    },
    "object-recognition": {
        "definition": "",
        "original_definition": [
            "Object recognition is a computer vision technique for detecting + classifying objects in images or videos. Since this is a combined task of object detection plus image classification, the state-of-the-art tables are recorded for each component task here and here.",
            "( Image credit: Tensorflow Object Detection API )"
        ],
        "original_definition_source": [
            "https://github.com/tensorflow/models/tree/master/research/object_detection"
        ],
        "subtasks": [
            "3D Object Recognition",
            "Continuous Object Recognition",
            "Depiction Invariant Object Recognition"
        ]
    },
    "3d-shape-reconstruction": {
        "definition": "",
        "original_definition": [
            "Image credit: GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision , ECCV'20"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Image Super-Resolution",
            "3D Shape Reconstruction From A Single 2D Image"
        ]
    },
    "3d-object-super-resolution": {
        "definition": "",
        "original_definition": [
            "3D object super-resolution is the task of up-sampling 3D objects.",
            "( Image credit: Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation )"
        ],
        "original_definition_source": [
            "https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation"
        ],
        "subtasks": [
            "Image Super-Resolution"
        ]
    },
    "image-captioning": {
        "definition": "",
        "original_definition": [
            "Image Captioning is the task of describing the content of an image in words. This task lies at the intersection of computer vision and natural language processing. Most image captioning systems use an encoder-decoder framework, where an input image is encoded into an intermediate representation of the information in the image, and then decoded into a descriptive text sequence. The most popular benchmarks are nocaps and COCO, and models are typically evaluated according to a BLEU or CIDER metric.",
            "( Image credit: Reflective Decoding Network for Image Captioning, ICCV'19 )"
        ],
        "original_definition_source": [
            "https://openaccess.thecvf.com/content_ICCV_2019/papers/Ke_Reflective_Decoding_Network_for_Image_Captioning_ICCV_2019_paper.pdf"
        ],
        "subtasks": [
            "3D dense captioning",
            "Relational Captioning",
            "Aesthetic Image Captioning",
            "Semi Supervised Learning for Image Captioning"
        ]
    },
    "action-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Skeleton Based Action Recognition",
            "Human Activity Recognition",
            "Online Action Detection",
            "Audio-Visual Active Speaker Detection"
        ]
    },
    "3d-object-detection": {
        "definition": "",
        "original_definition": [
            "2D object detection classifies the object category and estimates oriented 2D bounding boxes of physical objects from 3D sensor data.",
            "( Image credit: AVOD )"
        ],
        "original_definition_source": [
            "https://github.com/kujason/avod"
        ],
        "subtasks": [
            "Monocular 3D Object Detection"
        ]
    },
    "image-restoration": {
        "definition": "",
        "original_definition": [
            "Image Restoration is a family of inverse problems for obtaining a high quality image from a corrupted input image. Corruption may occur due to the image-capture process (e.g., noise, lens blur), post-processing (e.g., JPEG compression), or photography in non-ideal conditions (e.g., haze, motion blur).",
            "Source: Blind Image Restoration without Prior Knowledge"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2003.01764"
        ],
        "subtasks": [
            "Demosaicking",
            "Spectral Reconstruction",
            "JPEG Artifact Correction",
            "Underwater Image Restoration"
        ]
    },
    "3d-human-pose-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Monocular 3D Human Pose Estimation",
            "Pose Prediction",
            "3D Multi-Person Pose Estimation",
            "Weakly-supervised 3D Human Pose Estimation"
        ]
    },
    "scene-understanding": {
        "definition": "",
        "original_definition": [
            "Scene Understanding is something that to understand a scene. For instance, iPhone has function that help eye disabled person to take a photo by discribing what the camera sees. This is an example of Scene Understanding."
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Room Layouts From A Single RGB Panorama",
            "road scene understanding",
            "Monocular Cross-View Road Scene Parsing(Road)",
            "Outdoor Light Source Estimation"
        ]
    },
    "continuous-control": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Car Racing",
            "Steering Control",
            "Drone Controller"
        ]
    },
    "video-object-segmentation": {
        "definition": "",
        "original_definition": [
            "Video object segmentation is a binary labeling problem aiming to separate foreground object(s) from the background region of a video.",
            "For leaderboards please refer to the different subtasks."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Semi-Supervised Video Object Segmentation",
            "Unsupervised Video Object Segmentation",
            "Video Salient Object Detection",
            "Interactive Video Object Segmentation"
        ]
    },
    "3d-absolute-human-pose-estimation": {
        "definition": "",
        "original_definition": [
            "This task aims to solve absolute (camera-centric not root-relative) 3D human pose estimation.",
            "( Image credit: RootNet )"
        ],
        "original_definition_source": [
            "https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE"
        ],
        "subtasks": [
            "Deblurring",
            "3D Face Reconstruction",
            "Talking Head Generation",
            "3D Face Animation"
        ]
    },
    "multi-label-classification": {
        "definition": "",
        "original_definition": [
            "Multi-Label Classification is the supervised learning problem where an instance may be associated with multiple labels. This is an extension of single-label classification (i.e., multi-class, or binary) where each instance is only associated with a single class label.",
            "Source: Deep Learning for Multi-label Classification"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1502.05988"
        ],
        "subtasks": [
            "Medical Code Prediction",
            "Hierarchical Multi-label Classification"
        ]
    },
    "gesture-recognition": {
        "definition": "",
        "original_definition": [
            "Gesture Recognition is an active field of research with applications such as automatic recognition of sign language, interaction of humans and robots or for new ways of controlling video games.",
            "Source: Gesture Recognition in RGB Videos Using Human Body Keypoints and Dynamic Time Warping"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1906.12171"
        ],
        "subtasks": [
            "Skeleton Based Action Recognition",
            "Hand Gesture Recognition",
            "Hand-Gesture Recognition",
            "RF-based Gesture Recognition"
        ]
    },
    "3d-face-reconstruction": {
        "definition": "",
        "original_definition": [
            "3D face reconstruction is the task of reconstructing a face from an image into a 3D form (or mesh).",
            "( Image credit: 3DDFA_V2 )"
        ],
        "original_definition_source": [
            "https://github.com/cleardusk/3DDFA_V2"
        ],
        "subtasks": [
            "Deblurring",
            "Facial Recognition and Modelling"
        ]
    },
    "trajectory-prediction": {
        "definition": "",
        "original_definition": [
            "Trajectory Prediction is the problem of predicting the short-term (1-3 seconds) and long-term (3-5 seconds) spatial coordinates of various road-agents such as cars, buses, pedestrians, rickshaws, and animals, etc. These road-agents have different dynamic behaviors that may correspond to aggressive or conservative driving styles.",
            "Source: Forecasting Trajectory and Behavior of Road-Agents Using Spectral Clustering in Graph-LSTMs"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1912.01118"
        ],
        "subtasks": [
            "Trajectory Forecasting",
            "Human motion prediction"
        ]
    },
    "image-enhancement": {
        "definition": "",
        "original_definition": [
            "Image Enhancement is basically improving the interpretability or perception of information in images for human viewers and providing \u2018better\u2019 input for other automated image processing techniques. The principal objective of Image Enhancement is to modify attributes of an image to make it more suitable for a given task and a specific observer.",
            "Source: A Comprehensive Review of Image Enhancement Techniques"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1003.4053"
        ],
        "subtasks": [
            "Low-Light Image Enhancement",
            "Image Relighting",
            "De-aliasing",
            "Local Color Enhancement"
        ]
    },
    "object-localization": {
        "definition": "",
        "original_definition": [
            "Object Localization is the task of locating an instance of a particular object category in an image, typically by specifying a tightly cropped bounding box centered on the instance. An object proposal specifies a candidate bounding box, and an object proposal is said to be a correct localization if it sufficiently overlaps a human-labeled \u201cground-truth\u201d bounding box for the given object. In the literature, the \u201cObject Localization\u201d task is to locate one instance of an object category, whereas \u201cobject detection\u201d focuses on locating all instances of a category in a given image.",
            "Source: Fast On-Line Kernel Density Estimation for Active Object Localization"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1611.05369"
        ],
        "subtasks": [
            "Weakly-Supervised Object Localization",
            "Image-Based Localization",
            "Monocular 3D Object Localization",
            "Active Object Localization"
        ]
    },
    "imputation": {
        "definition": "",
        "original_definition": [
            "Substituting missing data with values according to some criteria."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Multivariate Time Series Imputation"
        ]
    },
    "saliency-detection": {
        "definition": "",
        "original_definition": [
            "Saliency Detection is a preprocessing step in computer vision which aims at finding salient objects in an image.",
            "Source: An Unsupervised Game-Theoretic Approach to Saliency Detection"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1708.02476"
        ],
        "subtasks": [
            "Saliency Prediction",
            "Co-Salient Object Detection",
            "Video Saliency Detection"
        ]
    },
    "image-inpainting": {
        "definition": "",
        "original_definition": [
            "Image Inpainting is a task of reconstructing missing regions in an image. It is an important problem in computer vision and an essential functionality in many imaging and graphics applications, e.g. object removal, image restoration, manipulation, re-targeting, compositing, and image-based rendering.",
            "Source: High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling",
            "Image source: High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2005.11742"
        ],
        "subtasks": [
            "Facial Inpainting",
            "Image Outpainting",
            "Cloud Removal",
            "Fine-Grained Image Inpainting"
        ]
    },
    "deblurring": {
        "definition": "",
        "original_definition": [
            "( Image credit: Deblurring Face Images using Uncertainty Guided Multi-Stream Semantic Networks )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1907.13106v1.pdf"
        ],
        "subtasks": [
            "Blind Image Deblurring",
            "Single-Image Blind Deblurring"
        ]
    },
    "video-semantic-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Camera shot segmentation"
        ]
    },
    "image-quality-assessment": {
        "definition": "",
        "original_definition": [
            "paper:Blind image quality assessment by visual neuron matrix code:https://github.com/Xiaodong-Bi/VNM"
        ],
        "original_definition_source": [],
        "subtasks": [
            "No-Reference Image Quality Assessment",
            "Blind Image Quality Assessment",
            "Aesthetics Quality Assessment",
            "Stereoscopic image quality assessment"
        ]
    },
    "action-recognition-in-still-images": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Image Inpainting"
        ]
    },
    "salient-object-detection": {
        "definition": "",
        "original_definition": [
            "RGB Salient object detection is a task-based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images.",
            "( Image credit: Attentive Feedback Network for Boundary-Aware Salient Object Detection )"
        ],
        "original_definition_source": [
            "http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Attentive_Feedback_Network_for_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf"
        ],
        "subtasks": [
            "Video Salient Object Detection",
            "Co-Salient Object Detection"
        ]
    },
    "out-of-distribution-detection": {
        "definition": "",
        "original_definition": [
            "Detect out-of-distribution or anomalous examples."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "ensemble-learning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "visual-tracking": {
        "definition": "",
        "original_definition": [
            "Visual Tracking is an essential and actively researched problem in the field of computer vision with various real-world applications such as robotic services, smart surveillance systems, autonomous driving, and human-computer interaction. It refers to the automatic estimation of the trajectory of an arbitrary target object, usually specified by a bounding box in the first frame, as it moves around in subsequent video frames.",
            "Source: Learning Reinforced Attentional Representation for End-to-End Visual Tracking"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1908.10009"
        ],
        "subtasks": [
            "Real-Time Visual Tracking",
            "Rgb-T Tracking",
            "RF-based Visual Tracking"
        ]
    },
    "image-to-image-translation": {
        "definition": "",
        "original_definition": [
            "Image-to-image translation is the task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain.",
            "( Image credit: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1703.10593v6.pdf"
        ],
        "subtasks": [
            "Unsupervised Image-To-Image Translation",
            "Synthetic-to-Real Translation",
            "Multimodal Unsupervised Image-To-Image Translation",
            "Fundus to Angiography Generation"
        ]
    },
    "image-dehazing": {
        "definition": "",
        "original_definition": [
            "( Image credit: Densely Connected Pyramid Dehazing Network )"
        ],
        "original_definition_source": [
            "https://github.com/hezhangsprinter/DCPDN"
        ],
        "subtasks": [
            "Real-Time Object Detection"
        ]
    },
    "image-registration": {
        "definition": "",
        "original_definition": [
            "Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements.",
            "Source: Image registration | Wikipedia",
            "( Image credit: Kornia )"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-compression": {
        "definition": "",
        "original_definition": [
            "Image Compression is an application of data compression for digital images to lower their storage and/or transmission requirements.",
            "Source: Variable Rate Deep Image Compression With a Conditional Autoencoder"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1909.04802"
        ],
        "subtasks": [
            "Jpeg Compression Artifact Reduction",
            "Lossy-Compression Artifact Reduction",
            "Color Image Compression Artifact Reduction"
        ]
    },
    "motion-estimation": {
        "definition": "",
        "original_definition": [
            "Motion Estimation is used to determine the block-wise or pixel-wise motion vectors between two frames.",
            "Source: MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1810.08768"
        ],
        "subtasks": []
    },
    "change-detection": {
        "definition": "",
        "original_definition": [
            "Image credit: \"A TRANSFORMER-BASED SIAMESE NETWORK FOR CHANGE DETECTION\""
        ],
        "original_definition_source": [],
        "subtasks": [
            "Semi-supervised Change Detection"
        ]
    },
    "visual-odometry": {
        "definition": "",
        "original_definition": [
            "Visual Odometry is an important area of information fusion in which the central aim is to estimate the pose of a robot using data collected by visual sensors.",
            "Source: Bi-objective Optimization for Robust RGB-D Visual Odometry"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1411.7445"
        ],
        "subtasks": [
            "Face Anti-Spoofing",
            "Monocular Visual Odometry"
        ]
    },
    "hand": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Hand Pose Estimation",
            "Hand Gesture Recognition",
            "Hand-Gesture Recognition",
            "Hand Segmentation"
        ]
    },
    "explainable-artificial-intelligence": {
        "definition": "",
        "original_definition": [
            "XAI refers to methods and techniques in the application of artificial intelligence (AI) such that the results of the solution can be understood by humans. It contrasts with the concept of the \"black box\" in machine learning where even its designers cannot explain why an AI arrived at a specific decision. XAI may be an implementation of the social right to explanation. XAI is relevant even if there is no legal right or regulatory requirement\u2014for example, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. This way the aim of XAI is to explain what has been done, what is done right now, what will be done next and unveil the information the actions are based on. These characteristics make it possible (i) to confirm existing knowledge (ii) to challenge existing knowledge and (iii) to generate new assumptions."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Explainable Models",
            "Explanation Fidelity Evaluation",
            "FAD Curve Analysis"
        ]
    },
    "image-clustering": {
        "definition": "",
        "original_definition": [
            "Models that partition the dataset into semantically meaningful clusters without having access to the ground truth labels.",
            "Image credit: ImageNet clustering results of SCAN: Learning to Classify Images without Labels (ECCV 2020)"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2005.12320"
        ],
        "subtasks": [
            "Face Clustering",
            "Online Clustering",
            "Multi-view Subspace Clustering",
            "Multi-modal Subspace Clustering"
        ]
    },
    "visual-reasoning": {
        "definition": "",
        "original_definition": [
            "Ability to understand actions and reasoning associated with any visual images"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Visual Commonsense Reasoning"
        ]
    },
    "human-interaction-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Skeleton Based Action Recognition",
            "One-Shot 3D Action Recognition",
            "Mutual Gaze"
        ]
    },
    "video-captioning": {
        "definition": "",
        "original_definition": [
            "Video Captioning is a task of automatic captioning a video by understanding the action and event in the video which can help in the retrieval of the video efficiently through text.",
            "Source: NITS-VC System for VATEX Video Captioning Challenge 2020"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2006.04058"
        ],
        "subtasks": [
            "Dense Video Captioning",
            "Visual Text Correction",
            "Audio-Visual Video Captioning"
        ]
    },
    "3d-point-cloud-classification": {
        "definition": "",
        "original_definition": [
            "Image: Qi et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Object Classification"
        ]
    },
    "point-cloud-registration": {
        "definition": "",
        "original_definition": [
            "Point Cloud Registration is a fundamental problem in 3D computer vision and photogrammetry. Given several sets of points in different coordinate systems, the aim of registration is to find the transformation that best aligns all of them into a common coordinate system. Point Cloud Registration plays a significant role in many vision applications such as 3D model reconstruction, cultural heritage management, landslide monitoring and solar energy analysis.",
            "Source: Iterative Global Similarity Points : A robust coarse-to-fine integration solution for pairwise 3D point cloud registration"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1808.03899"
        ],
        "subtasks": [
            "Image to Point Cloud Registration"
        ]
    },
    "action-classification": {
        "definition": "",
        "original_definition": [
            "Image source: The Kinetics Human Action Video Dataset"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Skeleton Based Action Recognition"
        ]
    },
    "hand-gesture-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Skeleton Based Action Recognition"
        ]
    },
    "medical-diagnosis": {
        "definition": "",
        "original_definition": [
            "Medical Diagnosis is the process of identifying the disease a patient is affected by, based on the assessment of specific risk factors, signs, symptoms and results of exams.",
            "Source: A probabilistic network for the diagnosis of acute cardiopulmonary diseases"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1609.06864"
        ],
        "subtasks": [
            "Retinal OCT Disease Classification",
            "alzheimer's disease detection",
            "Thoracic Disease Classification",
            "Blood Cell Count"
        ]
    },
    "colorization": {
        "definition": "",
        "original_definition": [
            "Colorization is the process of adding plausible color information to monochrome photographs or videos. Colorization is a highly undetermined problem, requiring mapping a real-valued luminance image to a three-dimensional color-valued one, that has not a unique solution.",
            "Source: ChromaGAN: An Adversarial Approach for Picture Colorization"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1907.09837"
        ],
        "subtasks": [
            "Line Art Colorization"
        ]
    },
    "3d-character-animation-from-a-single-photo": {
        "definition": "",
        "original_definition": [
            "Image: Weng et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Face Reconstruction",
            "Scene Recognition"
        ]
    },
    "image-manipulation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-semantic-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Real-Time 3D Semantic Segmentation",
            "furniture segmentation"
        ]
    },
    "dehazing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Image Dehazing",
            "Single Image Dehazing"
        ]
    },
    "stereo-matching": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "robot-navigation": {
        "definition": "",
        "original_definition": [
            "The fundamental objective of mobile Robot Navigation is to arrive at a goal position without collision. The mobile robot is supposed to be aware of obstacles and move freely in different working scenarios.",
            "Source: Learning to Navigate from Simulation via Spatial and Semantic Information Synthesis with Noise Model Embedding"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1910.05758"
        ],
        "subtasks": [
            "PointGoal Navigation",
            "Sequential Place Learning",
            "VNLA"
        ]
    },
    "activity-prediction": {
        "definition": "",
        "original_definition": [
            "Predict human activities in videos"
        ],
        "original_definition_source": [],
        "subtasks": [
            "motion prediction",
            "Sequential skip prediction"
        ]
    },
    "rain-removal": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Single Image Deraining"
        ]
    },
    "whole-slide-images": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "crowds": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Crowd Counting",
            "Visual Crowd Analysis",
            "Group Detection In Crowds"
        ]
    },
    "compressive-sensing": {
        "definition": "",
        "original_definition": [
            "Compressive Sensing is a new signal processing framework for efficiently acquiring and reconstructing a signal that have a sparse representation in a fixed linear basis.",
            "Source: Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1411.2405"
        ],
        "subtasks": []
    },
    "novel-view-synthesis": {
        "definition": "",
        "original_definition": [
            "Synthesize a target image with an arbitrary target camera pose from given source images and their camera poses.",
            "( Image credit: Multi-view to Novel view: Synthesizing novel views with Self-Learned Confidence )"
        ],
        "original_definition_source": [
            "https://github.com/shaohua0116/Multiview2Novelview"
        ],
        "subtasks": []
    },
    "visual-localization": {
        "definition": "",
        "original_definition": [
            "Visual Localization is the problem of estimating the camera pose of a given image relative to a visual representation of a known scene.",
            "Source: Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1908.06387"
        ],
        "subtasks": []
    },
    "object-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "3D Object Reconstruction"
        ]
    },
    "human-object-interaction-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Affordance Recognition"
        ]
    },
    "visual-place-recognition": {
        "definition": "",
        "original_definition": [
            "Visual Place Recognition is the task of matching a view of a place with a different view of the same place taken at a different time.",
            "Source: Visual place recognition using landmark distribution descriptors",
            "Image credit: Visual place recognition using landmark distribution descriptors"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1608.04274"
        ],
        "subtasks": [
            "Indoor Localization"
        ]
    },
    "scene-text-detection": {
        "definition": "",
        "original_definition": [
            "Scene Text Detection is a task to detect text regions in the complex background and label them with bounding boxes.",
            "Source: ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene Text Detection"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2004.04940"
        ],
        "subtasks": [
            "Curved Text Detection",
            "Multi-Oriented Scene Text Detection"
        ]
    },
    "3d-car-instance-understanding": {
        "definition": "",
        "original_definition": [
            "3D Car Instance Understanding is the task of estimating properties (e.g.translation, rotation and shape) of a moving or parked vehicle on the road.",
            "( Image credit: Occlusion-Net )"
        ],
        "original_definition_source": [
            "http://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf"
        ],
        "subtasks": [
            "Text-to-Image Generation",
            "Hand Gesture Recognition"
        ]
    },
    "3d-scene-reconstruction": {
        "definition": "",
        "original_definition": [
            "Creating 3D scene either using conventional SFM pipelines or latest deep learning approaches."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Face Generation",
            "3D Semantic Scene Completion from a single RGB image"
        ]
    },
    "image-matching": {
        "definition": "",
        "original_definition": [
            "Image Matching or wide multiple baseline stereo (WxBS) is a process of establishing a sufficient number of pixel or region correspondences from two or more images depicting the same scene to estimate the geometric relationship between cameras, which produced these images.",
            "Source: The Role of Wide Baseline Stereo in the Deep Learning World",
            "( Image credit: Kornia )"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Semantic correspondence",
            "Patch Matching",
            "set matching",
            "Matching Disparate Images"
        ]
    },
    "hyperspectral": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Hyperspectral Image Classification",
            "Hyperspectral Unmixing",
            "Classification Of Hyperspectral Images",
            "Hyperspectral Image Segmentation"
        ]
    },
    "saliency-prediction": {
        "definition": "",
        "original_definition": [
            "Saliency prediction aims to predict important locations in a visual scene. It is a per-pixel regression task with predicted values ranging from 0 to 1.",
            "Benefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Saliency Prediction"
        ]
    },
    "point-cloud-classification": {
        "definition": "",
        "original_definition": [
            "Point Cloud Classification is a task involving the classification of unordered 3D point sets (point clouds)."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Jet Tagging"
        ]
    },
    "edge-detection": {
        "definition": "",
        "original_definition": [
            "Edge Detection is a fundamental image processing technique which involves computing an image gradient to quantify the magnitude and direction of edges in an image. Image gradients are used in various downstream tasks in computer vision such as line detection, feature detection, and image classification.",
            "Source: Artistic Enhancement and Style Transfer of Image Edges using Directional Pseudo-coloring",
            "( Image credit: Kornia )"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1906.07981"
        ],
        "subtasks": []
    },
    "keyword-spotting": {
        "definition": "",
        "original_definition": [
            "In speech processing, keyword spotting deals with the identification of keywords in utterances.",
            "( Image credit: Simon Grest )"
        ],
        "original_definition_source": [
            "https://github.com/simongrest/kaggle-freesound-audio-tagging-2019"
        ],
        "subtasks": [
            "Small-Footprint Keyword Spotting",
            "Visual Keyword Spotting"
        ]
    },
    "scene-classification": {
        "definition": "",
        "original_definition": [
            "Scene Classification is a task in which scenes from photographs are categorically classified. Unlike object classification, which focuses on classifying prominent objects in the foreground, Scene Classification uses the layout of objects within the scene, in addition to the ambient context, for classification.",
            "Source: Scene classification with Convolutional Neural Networks"
        ],
        "original_definition_source": [
            "http://cs231n.stanford.edu/reports/2017/pdfs/102.pdf"
        ],
        "subtasks": []
    },
    "document-text-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Learning with noisy labels",
            "Multi-Label Classification Of Biomedical Texts",
            "Political Salient Issue Orientation Detection"
        ]
    },
    "one-shot-learning": {
        "definition": "",
        "original_definition": [
            "One-shot learning is the task of learning information about object categories from a single training example.",
            "( Image credit: Siamese Neural Networks for One-shot Image Recognition )"
        ],
        "original_definition_source": [
            "https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
        ],
        "subtasks": []
    },
    "superpixels": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "boundary-detection": {
        "definition": "",
        "original_definition": [
            "Boundary Detection is a vital part of extracting information encoded in images, allowing for the computation of quantities of interest including density, velocity, pressure, etc.",
            "Source: A Locally Adapting Technique for Boundary Detection using Image Segmentation"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1707.09030"
        ],
        "subtasks": [
            "Junction Detection"
        ]
    },
    "image-matting": {
        "definition": "",
        "original_definition": [
            "Image Matting is the process of accurately estimating the foreground object in images and videos. It is a very important technique in image and video editing applications, particularly in film production for creating visual effects. In case of image segmentation, we segment the image into foreground and background by labeling the pixels. Image segmentation generates a binary image, in which a pixel either belongs to foreground or background. However, Image Matting is different from the image segmentation, wherein some pixels may belong to foreground as well as background, such pixels are called partial or mixed pixels. In order to fully separate the foreground from the background in an image, accurate estimation of the alpha values for partial or mixed pixels is necessary.",
            "Source: Automatic Trimap Generation for Image Matting",
            "Image Source: Real-Time High-Resolution Background Matting"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1707.00333",
            "https://arxiv.org/abs/1707.00333"
        ],
        "subtasks": [
            "Semantic Image Matting"
        ]
    },
    "point-cloud-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Point Cloud Completion"
        ]
    },
    "emotion-classification": {
        "definition": "",
        "original_definition": [
            "Given an input, classify it as 'neutral or no emotion' or as one, or more, of several given emotions that best represent the mental state of the writer."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "referring-expression": {
        "definition": "",
        "original_definition": [
            "Referring expressions places a bounding box around the instance corresponding to the provided description and image."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-object-reconstruction": {
        "definition": "",
        "original_definition": [
            "Image: Choy et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Face Reconstruction",
            "3D Object Reconstruction From A Single Image"
        ]
    },
    "video-summarization": {
        "definition": "",
        "original_definition": [
            "Video Summarization aims to generate a short synopsis that summarizes the video content by selecting its most informative and important parts. The produced summary is usually composed of a set of representative video frames (a.k.a. video key-frames), or video fragments (a.k.a. video key-fragments) that have been stitched in chronological order to form a shorter video. The former type of a video summary is known as video storyboard, and the latter type is known as video skim.",
            "Source: Video Summarization Using Deep Neural Networks: A Survey\nImage credit: iJRASET"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2101.06072",
            "https://arxiv.org/abs/2101.06072"
        ],
        "subtasks": [
            "Unsupervised Video Summarization",
            "Supervised Video Summarization"
        ]
    },
    "camera-calibration": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "remote-sensing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Remote Sensing Image Classification",
            "Change detection for remote sensing images",
            "Segmentation Of Remote Sensing Imagery",
            "The Semantic Segmentation Of Remote Sensing Imagery"
        ]
    },
    "video-question-answering": {
        "definition": "",
        "original_definition": [
            "Video Question Answering (VideoQA) aims to answer natural language questions according to the given videos. Given a video and a question in natural language, the model produces accurate answers according to the content of the video."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "document-layout-analysis": {
        "definition": "",
        "original_definition": [
            "\"Document Layout Analysis is performed to determine physical structure of a document, that is, to determine document components. These document components can consist of single connected components-regions [...] of pixels that are adjacent to form single regions [...] , or group of text lines. A text line is a group of characters, symbols, and words that are adjacent, \u201crelatively close\u201d to each other and through which a straight line can be drawn (usually with horizontal or vertical orientation).\" L. O'Gorman, \"The document spectrum for page layout analysis,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 15, no. 11, pp. 1162-1173, Nov. 1993.",
            "Image credit: PubLayNet: largest dataset ever for document layout analysis"
        ],
        "original_definition_source": [],
        "subtasks": [
            "MS-SSIM"
        ]
    },
    "face-reconstruction": {
        "definition": "",
        "original_definition": [
            "Face reconstruction is the task of recovering the facial geometry of a face from an image.",
            "( Image credit: Microsoft Deep3DFaceReconstruction )"
        ],
        "original_definition_source": [
            "https://github.com/Microsoft/Deep3DFaceReconstruction"
        ],
        "subtasks": [
            "3D Face Reconstruction"
        ]
    },
    "human-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "texture-synthesis": {
        "definition": "",
        "original_definition": [
            "The fundamental goal of example-based Texture Synthesis is to generate a texture, usually larger than the input, that faithfully captures all the visual characteristics of the exemplar, yet is neither identical to it, nor exhibits obvious unnatural looking artifacts.",
            "Source: Non-Stationary Texture Synthesis by Adversarial Expansion"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1805.04487"
        ],
        "subtasks": []
    },
    "facial-landmark-detection": {
        "definition": "",
        "original_definition": [
            "Facial landmark detection is the task of detecting key landmarks on the face and tracking them (being robust to rigid and non-rigid facial deformations due to head movements and facial expressions).",
            "( Image credit: Style Aggregated Network for Facial Landmark Detection )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1803.04108v4.pdf"
        ],
        "subtasks": [
            "Unsupervised Facial Landmark Detection",
            "3D Facial Landmark Localization"
        ]
    },
    "point-cloud-segmentation": {
        "definition": "",
        "original_definition": [
            "3D point cloud segmentation is the process of classifying point clouds into multiple homogeneous regions, the points in the same region will have the same properties. The segmentation is challenging because of high redundancy, uneven sampling density, and lack explicit structure of point cloud data. This problem has many applications in robotics such as intelligent vehicles, autonomous mapping and navigation.",
            "Source: 3D point cloud segmentation: A survey"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "classification": {
        "definition": "",
        "original_definition": [
            "Algorithms trying to solve the general task of classification."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "deepfake-detection": {
        "definition": "",
        "original_definition": [
            "DeepFakes involves videos, often obscene, in which a face can be swapped with someone else\u2019s using neural networks. DeepFakes are a general public concern, thus it's important to develop methods to detect them.",
            "Description source: DeepFakes: a New Threat to Face Recognition? Assessment and Detection",
            "Image source: DeepFakes: a New Threat to Face Recognition? Assessment and Detection"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Synthetic Speech Detection"
        ]
    },
    "reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "3D Human Reconstruction",
            "Single-View 3D Reconstruction",
            "Single-Image-Based Hdr Reconstruction"
        ]
    },
    "face-model": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-reconstruction": {
        "definition": "",
        "original_definition": [
            "Image: Gwak et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Point cloud reconstruction",
            "3D Semantic Scene Completion",
            "3D Room Layouts From A Single RGB Panorama",
            "3D Semantic Scene Completion from a single RGB image"
        ]
    },
    "2d-semantic-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Text Style Transfer",
            "Disjoint 15-1",
            "Disjoint 10-1",
            "Disjoint 15-5"
        ]
    },
    "depth-completion": {
        "definition": "",
        "original_definition": [
            "Visual Dialog requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content. Specifically, given an image, a dialog history, and a follow-up question about the image, the task is to answer the question."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-parsing": {
        "definition": "",
        "original_definition": [
            "Human parsing is the task of segmenting a human image into different fine-grained semantic parts such as head, torso, arms and legs.",
            "( Image credit: Multi-Human-Parsing (MHP) )"
        ],
        "original_definition_source": [
            "https://github.com/ZhaoJ9014/Multi-Human-Parsing"
        ],
        "subtasks": [
            "Multi-Human Parsing"
        ]
    },
    "video-instance-segmentation": {
        "definition": "",
        "original_definition": [
            "The goal of video instance segmentation is simultaneous detection, segmentation and tracking of instances in videos. In words, it is the first time that the image instance segmentation problem is extended to the video domain.",
            "To facilitate research on this new task, a large-scale benchmark called YouTube-VIS, which consists of 2,883 high-resolution YouTube videos, a 40-category label set and 131k high-quality instance masks is built."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-multi-person-pose-estimation": {
        "definition": "",
        "original_definition": [
            "This task aims to solve root-relative 3D multi-person pose estimation. No human bounding box and root joint coordinate groundtruth are used in testing time.",
            "( Image credit: RootNet )"
        ],
        "original_definition_source": [
            "https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE"
        ],
        "subtasks": [
            "3D Multi-Person Pose Estimation (root-relative)",
            "3D Multi-Person Pose Estimation (absolute)",
            "3D Multi-Person Mesh Recovery"
        ]
    },
    "privacy-preserving-deep-learning": {
        "definition": "",
        "original_definition": [
            "The goal of privacy-preserving (deep) learning is to train a model while preserving privacy of the training dataset. Typically, it is understood that the trained model should be privacy-preserving (e.g., due to the training algorithm being differentially private)."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Membership Inference Attack"
        ]
    },
    "gaze-estimation": {
        "definition": "",
        "original_definition": [
            "Gaze Estimation is a task to predict where a person is looking at given the person\u2019s full face. The task contains two directions: 3-D gaze vector and 2-D gaze position estimation. 3-D gaze vector estimation is to predict the gaze vector, which is usually used in the automotive safety. 2-D gaze position estimation is to predict the horizontal and vertical coordinates on a 2-D screen, which allows utilizing gaze point to control a cursor for human-machine interaction.",
            "Source: A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1910.07331"
        ],
        "subtasks": []
    },
    "keypoint-detection": {
        "definition": "",
        "original_definition": [
            "Keypoint detection involves simultaneously detecting people and localizing their keypoints. Keypoints are the same thing as interest points. They are spatial locations, or points in the image that define what is interesting or what stand out in the image. They are invariant to image rotation, shrinkage, translation, distortion, and so on.",
            "( Image credit: PifPaf: Composite Fields for Human Pose Estimation; \"Learning to surf\" by fotologic, license: CC-BY-2.0 )"
        ],
        "original_definition_source": [
            "https://github.com/vita-epfl/openpifpaf"
        ],
        "subtasks": [
            "2D Human Pose Estimation"
        ]
    },
    "unity": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "disease-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Retinal OCT Disease Classification",
            "Disease Trajectory Forecasting"
        ]
    },
    "hand-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Hand pose estimation is the task of finding the joints of the hand from an image or set of video frames.",
            "( Image credit: Pose-REN )"
        ],
        "original_definition_source": [
            "https://github.com/xinghaochen/Pose-REN"
        ],
        "subtasks": [
            "3D Hand Pose Estimation"
        ]
    },
    "weakly-supervised-segmentation": {
        "definition": "",
        "original_definition": [
            "Few-shot semantic segmentation (FSS) learns to segment target objects in query image given few pixel-wise annotated support image."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "interest-point-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Homography Estimation"
        ]
    },
    "temporal-localization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Language-Based Temporal Localization"
        ]
    },
    "activity-detection": {
        "definition": "",
        "original_definition": [
            "Detecting activities in extended videos."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-view-learning": {
        "definition": "",
        "original_definition": [
            "Multi-View Learning is a machine learning framework where data are represented by multiple distinct feature groups, and each feature group is referred to as a particular view.",
            "Source: Dissimilarity-based representation for radiomics applications"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1803.04460"
        ],
        "subtasks": [
            "Incomplete multi-view clustering"
        ]
    },
    "few-shot-object-detection": {
        "definition": "",
        "original_definition": [
            "Target: To detect objects of novel categories with just a few training samples.",
            "A clear explanation of the few-shot object detection task and its differences with few-shot classification can be found in \"A Survey of Self-Supervised and Few-Shot Object Detection\": https://gabrielhuang.github.io/fsod-survey/"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "depth-and-camera-motion": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Face Anti-Spoofing"
        ]
    },
    "motion-segmentation": {
        "definition": "",
        "original_definition": [
            "Motion Segmentation is an essential task in many applications in Computer Vision and Robotics, such as surveillance, action recognition and scene understanding. The classic way to state the problem is the following: given a set of feature points that are tracked through a sequence of images, the goal is to cluster those trajectories according to the different motions they belong to. It is assumed that the scene contains multiple objects that are moving rigidly and independently in 3D-space.",
            "Source: Robust Motion Segmentation from Pairwise Matches"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.09043"
        ],
        "subtasks": []
    },
    "template-matching": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "virtual-try-on": {
        "definition": "",
        "original_definition": [
            "Virtual try-on of clothing or other items such as glasses and makeup. Most recent techniques use Generative Adversarial Networks."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "pose-tracking": {
        "definition": "",
        "original_definition": [
            "Pose Tracking is the task of estimating multi-person human poses in videos and assigning unique instance IDs for each keypoint across frames. Accurate estimation of human keypoint-trajectories is useful for human action recognition, human interaction understanding, motion capture and animation.",
            "Source: LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.02822"
        ],
        "subtasks": [
            "3D Human Pose Tracking"
        ]
    },
    "disparity-estimation": {
        "definition": "",
        "original_definition": [
            "The Disparity Estimation is the task of finding the pixels in the multiscopic views that correspond to the same 3D point in the scene."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "scene-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Fine-Grained Image Recognition",
            "License Plate Recognition"
        ]
    },
    "intelligent-surveillance": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Vehicle Re-Identification"
        ]
    },
    "vehicle-re-identification": {
        "definition": "",
        "original_definition": [
            "Vehicle re-identification is the task of identifying the same vehicle across multiple cameras.",
            "( Image credit: A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras )"
        ],
        "original_definition_source": [
            "https://github.com/icarofua/siamese-two-stream"
        ],
        "subtasks": []
    },
    "face-generation": {
        "definition": "",
        "original_definition": [
            "Face generation is the task of generating (or interpolating) new faces from an existing dataset.",
            "The state-of-the-art results for this task are located in the Image Generation parent.",
            "( Image credit: Progressive Growing of GANs for Improved Quality, Stability, and Variation )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1710.10196v3.pdf"
        ],
        "subtasks": [
            "Talking Face Generation",
            "Talking Head Generation",
            "Face Age Editing"
        ]
    },
    "scene-flow-estimation": {
        "definition": "",
        "original_definition": [
            "Scene Flow Estimation is the task of obtaining 3D structure and 3D motion of dynamic scenes, which is crucial to environment perception, e.g., in the context of autonomous navigation.",
            "Source: Self-Supervised Monocular Scene Flow Estimation"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2004.04143"
        ],
        "subtasks": []
    },
    "sign-language-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "camera-localization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Camera Relocalization"
        ]
    },
    "object-discovery": {
        "definition": "",
        "original_definition": [
            "Object Discovery is the task of identifying previously unseen objects.",
            "Source: Unsupervised Object Discovery and Segmentation of RGBD-images"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1710.06929"
        ],
        "subtasks": []
    },
    "deep-attention": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "motion-synthesis": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "motion style transfer"
        ]
    },
    "future-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "autonomous-driving": {
        "definition": "",
        "original_definition": [
            "Autonomous driving is the task of driving a vehicle without human conduction.",
            "Many of the state-of-the-art results can be found at more general task pages such as 3D Object Detection and Semantic Segmentation.",
            "(Image credit: Exploring the Limitations of Behavior Cloning for Autonomous Driving)"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1904.08980v1.pdf"
        ],
        "subtasks": [
            "Motion Forecasting",
            "CARLA MAP Leaderboard",
            "Dead-Reckoning Prediction"
        ]
    },
    "color-constancy": {
        "definition": "",
        "original_definition": [
            "Color Constancy is the ability of the human vision system to perceive the colors of the objects in the scene largely invariant to the color of the light source. The task of computational Color Constancy is to estimate the scene illumination and then perform the chromatic adaptation in order to remove the influence of the illumination color on the colors of the objects in the scene.",
            "Source: CroP: Color Constancy Benchmark Dataset Generator"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1903.12581"
        ],
        "subtasks": [
            "Few-Shot Camera-Adaptive Color Constancy"
        ]
    },
    "video-retrieval": {
        "definition": "",
        "original_definition": [
            "RGB Salient object detection is a task-based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images.",
            "( Image credit: Attentive Feedback Network for Boundary-Aware Salient Object Detection )"
        ],
        "original_definition_source": [
            "http://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Attentive_Feedback_Network_for_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf"
        ],
        "subtasks": [
            "Video Salient Object Detection",
            "Co-Salient Object Detection"
        ]
    },
    "interactive-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-quality-assessment": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "motion-forecasting": {
        "definition": "",
        "original_definition": [
            "Motion forecasting is the task of predicting the location of a tracked object in the future"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Multiple Object Forecasting"
        ]
    },
    "matlab": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Code Search"
        ]
    },
    "scene-segmentation": {
        "definition": "",
        "original_definition": [
            "Scene segmentation is the task of splitting a scene into its various object components.",
            "Image adapted from Temporally coherent 4D reconstruction of complex dynamic scenes."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Thermal Image Segmentation"
        ]
    },
    "text-spotting": {
        "definition": "",
        "original_definition": [
            "The ability to read text in natural scenes"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-categorization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Fine-Grained Visual Categorization"
        ]
    },
    "object-counting": {
        "definition": "",
        "original_definition": [
            "The goal of Object Counting task is to count the number of object instances in a single image or video sequence. It has many real-world applications such as traffic flow monitoring, crowdedness estimation, and product counting.",
            "Source: Learning to Count Objects with Few Exemplar Annotations"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.07898"
        ],
        "subtasks": []
    },
    "semi-supervised-video-object-segmentation": {
        "definition": "",
        "original_definition": [
            "The semi-supervised scenario assumes the user inputs a full mask of the object(s) of interest in the first frame of a video sequence. Methods have to produce the segmentation mask for that object(s) in the subsequent frames."
        ],
        "original_definition_source": [],
        "subtasks": [
            "One-shot visual object segmentation"
        ]
    },
    "referring-expression-segmentation": {
        "definition": "",
        "original_definition": [
            "The task aims at labelling the pixels of an image or video that represent an object instance referred by a linguistic expression. In particular, the referring expression (RE) must allow the identification of an indivisual object in a discourse or scene (the referent). REs unambiguosly identify the target instace."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "gait-recognition": {
        "definition": "",
        "original_definition": [
            "( Image credit: GaitSet: Regarding Gait as a Set for Cross-View Gait Recognition )"
        ],
        "original_definition_source": [
            "https://github.com/AbnerHqC/GaitSet"
        ],
        "subtasks": [
            "Multiview Gait Recognition",
            "Gait Recognition in the Wild"
        ]
    },
    "image-cropping": {
        "definition": "",
        "original_definition": [
            "Image Cropping is a common photo manipulation process, which improves the overall composition by removing unwanted regions. Image Cropping is widely used in photographic, film processing, graphic design, and printing businesses.",
            "Source: Listwise View Ranking for Image Cropping"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.05352"
        ],
        "subtasks": []
    },
    "multi-label-image-classification": {
        "definition": "",
        "original_definition": [
            "The Multi-Label Image Classification focuses on predicting labels for images in a multi-class classification problem where each image may belong to more than one class."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "texture-classification": {
        "definition": "",
        "original_definition": [
            "Texture Classification is a fundamental issue in computer vision and image processing, playing a significant role in many applications such as medical image analysis, remote sensing, object recognition, document analysis, environment modeling, content-based image retrieval and many more.",
            "Source: Improving Texture Categorization with Biologically Inspired Filtering"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1312.0072"
        ],
        "subtasks": []
    },
    "weakly-supervised-action-localization": {
        "definition": "",
        "original_definition": [
            "In this task, the training data consists of videos with a list of activities in them without any temporal boundary annotations. However, while testing, given a video, the algorithm should recognize the activities in the video and also provide the start and end time."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "automatic-post-editing": {
        "definition": "",
        "original_definition": [
            "Automatic post-editing (APE) is used to correct errors in the translation made by the machine translation systems."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "person-search": {
        "definition": "",
        "original_definition": [
            "Person Search is a task which aims at matching a specific person among a great number of whole scene images.",
            "Source: Re-ID Driven Localization Refinement for Person Search"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1909.08580"
        ],
        "subtasks": []
    },
    "natural-language-transduction": {
        "definition": "",
        "original_definition": [
            "Converting one sequence into another"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Lipreading"
        ]
    },
    "shadow-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Shadow Detection And Removal"
        ]
    },
    "visual-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Fine-Grained Visual Recognition"
        ]
    },
    "image-forensics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-enhancement": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-inpainting": {
        "definition": "",
        "original_definition": [
            "The goal of Video Inpainting is to fill in missing regions of a given video sequence with contents that are both spatially and temporally coherent. Video Inpainting, also known as video completion, has many real-world applications such as undesired object removal and video restoration.",
            "Source: Deep Flow-Guided Video Inpainting"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1905.02884"
        ],
        "subtasks": []
    },
    "multi-object-tracking": {
        "definition": "",
        "original_definition": [
            "Multiple Object Tracking is the problem of automatically identifying multiple objects in a video and representing them as a set of trajectories with high accuracy."
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Multi-Object Tracking",
            "Real-Time Multi-Object Tracking",
            "Multi-Animal Tracking with identification"
        ]
    },
    "mixed-reality": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "object-categorization": {
        "definition": "",
        "original_definition": [
            "Object categorization identifies which label, from a given set, best corresponds to an image region defined by an input image and bounding box."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "probabilistic-deep-learning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "lipreading": {
        "definition": "",
        "original_definition": [
            "Lipreading is a process of extracting speech by watching lip movements of a speaker in the absence of sound. Humans lipread all the time without even noticing. It is a big part in communication albeit not as dominant as audio. It is a very helpful skill to learn especially for those who are hard of hearing.",
            "Deep Lipreading is the process of extracting speech from a video of a silent talking face using deep neural networks. It is also known by few other names: Visual Speech Recognition (VSR), Machine Lipreading, Automatic Lipreading etc.",
            "The primary methodology involves two stages: i) Extracting visual and temporal features from a sequence of image frames from a silent talking video ii) Processing the sequence of features into units of speech e.g. characters, words, phrases etc. We can find several implementations of this methodology either done in two separate stages or trained end-to-end in one go."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "sketch": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Face Sketch Synthesis",
            "Sketch Recognition",
            "Photo-To-Caricature Translation",
            "Drawing Pictures"
        ]
    },
    "breast-cancer-histology-image-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Breast Cancer Detection",
            "Breast Cancer Histology Image Classification (20% labels)"
        ]
    },
    "geometric-matching": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "line-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "vision-language-navigation": {
        "definition": "",
        "original_definition": [
            "Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments.",
            "( Image credit: Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1904.04195v1.pdf"
        ],
        "subtasks": []
    },
    "transparent-object-detection": {
        "definition": "",
        "original_definition": [
            "Detecting transparent objects in 2D or 3D"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Transparent objects"
        ]
    },
    "document-image-classification": {
        "definition": "",
        "original_definition": [
            "Document image classification is the task of classifying documents based on images of their contents.",
            "( Image credit: Real-Time Document Image Classification using Deep CNN and Extreme Learning Machines )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1711.05862v1.pdf"
        ],
        "subtasks": []
    },
    "reflection-removal": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "iris-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "moment-retrieval": {
        "definition": "",
        "original_definition": [
            "Moment retrieval can de defined as the task of \"localizing moments in a video given a user query\".",
            "Description from: QVHIGHLIGHTS: Detecting Moments and Highlights in Videos via Natural Language Queries",
            "Image credit: QVHIGHLIGHTS: Detecting Moments and Highlights in Videos via Natural Language Queries"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "zero-shot-action-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-face-modeling": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Facial Recognition and Modelling"
        ]
    },
    "simultaneous-localization-and-mapping": {
        "definition": "",
        "original_definition": [
            "Simultaneous localization and mapping (SLAM) is the task of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.",
            "( Image credit: ORB-SLAM2 )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1610.06475v2.pdf"
        ],
        "subtasks": [
            "Semantic SLAM",
            "Object SLAM"
        ]
    },
    "talking-head-generation": {
        "definition": "",
        "original_definition": [
            "Talking head generation is the task of generating a talking face from a set of images of a person.",
            "( Image credit: Few-Shot Adversarial Learning of Realistic Neural Talking Head Models )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1905.08233v2.pdf"
        ],
        "subtasks": [
            "Unconstrained Lip-synchronization"
        ]
    },
    "3d-multi-object-tracking": {
        "definition": "",
        "original_definition": [
            "Image: Weng et al"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "action-anticipation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "activity-recognition-in-videos": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Activity Prediction"
        ]
    },
    "lidar-semantic-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "surface-normals-estimation": {
        "definition": "",
        "original_definition": [
            "Surface normal estimation deals with the task of predicting the surface orientation of the objects present inside a scene. Refer to Designing Deep Networks for Surface Normal Estimation (Wang et al.) to get a good overview of several design choices that led to the development of a CNN-based surface normal estimator."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-restoration": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "semi-supervised-object-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "single-image-based-hdr-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Tone Mapping"
        ]
    },
    "video-reconstruction": {
        "definition": "",
        "original_definition": [
            "Source: Deep-SloMo"
        ],
        "original_definition_source": [
            "https://github.com/avinashpaliwal/Deep-SloMo"
        ],
        "subtasks": []
    },
    "viewpoint-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "action-understanding": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "event-based-vision": {
        "definition": "",
        "original_definition": [
            "An event camera, also known as a neuromorphic camera, silicon retina or dynamic vision sensor, is an imaging sensor that responds to local changes in brightness. Event cameras do not capture images using a shutter as conventional cameras do. Instead, each pixel inside an event camera operates independently and asynchronously, reporting changes in brightness as they occur and staying silent otherwise. Modern event cameras have microsecond temporal resolution, 120 dB dynamic range, and less under/overexposure and motion blur than frame cameras."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "infrared-and-visible-image-fusion": {
        "definition": "",
        "original_definition": [
            "Image fusion with paired infrared and visible images"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "line-segment-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "spoof-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Face Presentation Attack Detection",
            "Cross-Domain Iris Presentation Attack Detection",
            "Detecting Image Manipulation",
            "Finger Dorsal Image Spoof Detection"
        ]
    },
    "hdr-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Multi-Exposure Image Fusion"
        ]
    },
    "3d-object-tracking": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "abnormal-event-detection-in-video": {
        "definition": "",
        "original_definition": [
            "Abnormal Event Detection In Video is a challenging task in computer vision, as the definition of what an abnormal event looks like depends very much on the context. For instance, a car driving by on the street is regarded as a normal event, but if the car enters a pedestrian area, this is regarded as an abnormal event. A person running on a sports court (normal event) versus running outside from a bank (abnormal event) is another example. Although what is considered abnormal depends on the context, we can generally agree that abnormal events should be unexpected events that occur less often than familiar (normal) events",
            "Source: Unmasking the abnormal events in video",
            "Image: Ravanbakhsh et al"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1705.08182"
        ],
        "subtasks": [
            "Semi-supervised Anomaly Detection"
        ]
    },
    "action-quality-assessment": {
        "definition": "",
        "original_definition": [
            "Assessing/analyzing/quantifying how well an action was performed."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "caricature": {
        "definition": "",
        "original_definition": [
            "Caricature is a pictorial representation or description that deliberately exaggerates a person\u2019s distinctive features or peculiarities to create an easily identifiable visual likeness with a comic effect. This vivid art form contains the concepts of abstraction, simplification and exaggeration.",
            "Source: Alive Caricature from 2D to 3D"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1803.06802"
        ],
        "subtasks": []
    },
    "cloud-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "animation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Image Animation",
            "3D Character Animation From A Single Photo"
        ]
    },
    "content-based-image-retrieval": {
        "definition": "",
        "original_definition": [
            "Content-Based Image Retrieval is a well studied problem in computer vision, with retrieval problems generally divided into two groups: category-level retrieval and instance-level retrieval. Given a query image of the Sydney Harbour bridge, for instance, category-level retrieval aims to find any bridge in a given dataset of images, whilst instance-level retrieval must find the Sydney Harbour bridge to be considered a match.",
            "Source: Camera Obscurer: Generative Art for Design Inspiration"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1903.02165"
        ],
        "subtasks": [
            "Drone navigation",
            "Drone-view target localization"
        ]
    },
    "object-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Camouflaged Object Segmentation",
            "Landslide segmentation"
        ]
    },
    "shape-representation-of-3d-point-clouds": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "3D Point Cloud Reconstruction"
        ]
    },
    "cross-domain-few-shot": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "cross-domain few-shot learning"
        ]
    },
    "dense-pixel-correspondence-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "intrinsic-image-decomposition": {
        "definition": "",
        "original_definition": [
            "Intrinsic Image Decomposition is the process of separating an image into its formation components such as reflectance (albedo) and shading (illumination). Reflectance is the color of the object, invariant to camera viewpoint and illumination conditions, whereas shading, dependent on camera viewpoint and object geometry, consists of different illumination effects, such as shadows, shading and inter-reflections. Using intrinsic images, instead of the original images, can be beneficial for many computer vision algorithms. For instance, for shape-from-shading algorithms, the shading images contain important visual cues to recover geometry, while for segmentation and detection algorithms, reflectance images can be beneficial as they are independent of confounding illumination effects. Furthermore, intrinsic images are used in a wide range of computational photography applications, such as material recoloring, relighting, retexturing and stylization.",
            "Source: CNN based Learning using Reflection and Retinex Models for Intrinsic Image Decomposition"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1712.01056"
        ],
        "subtasks": []
    },
    "rotated-mnist": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-dynamics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "3D Human Dynamics"
        ]
    },
    "blind-face-restoration": {
        "definition": "",
        "original_definition": [
            "Blind face restoration aims at recovering high-quality faces from the low-quality counterparts suffering from unknown degradation, such as low-resolution, noise, blur, compression artifacts, etc. When applied to real-world scenarios, it becomes more challenging, due to more complicated degradation, diverse poses and expressions.",
            "Description source: Towards Real-World Blind Face Restoration with Generative Facial Prior",
            "Image source: Towards Real-World Blind Face Restoration with Generative Facial Prior"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "camouflaged-object-segmentation": {
        "definition": "",
        "original_definition": [
            "Camouflaged object segmentation (COS) or Camouflaged object detection (COD), which was originally promoted by T.-N. Le et al. (2017), aims to identify objects that conceal their texture into the surrounding environment. The high intrinsic similarities between the target object and the background make COS/COD far more challenging than the traditional object segmentation task. Also, refer to the online benchmarks on CAMO dataset, COD dataset, and online demo.",
            "( Image source: Anabranch Network for Camouflaged Object Segmentation )"
        ],
        "original_definition_source": [
            "https://www.sciencedirect.com/science/article/abs/pii/S1077314219300608"
        ],
        "subtasks": []
    },
    "human-part-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-action-generation": {
        "definition": "",
        "original_definition": [
            "Yan et al. (2019) CSGN:",
            "\"When the dancer is stepping, jumping and spinning on the stage, attentions of all audiences are attracted by the streamof the fluent and graceful movements. Building a model that is capable of dancing is as fascinating a task as appreciating the performance itself. In this paper, we aim to generate long-duration human actions represented as skeleton sequences, e.g. those that cover the entirety of a dance, with hundreds of moves and countless possible combinations.\"",
            "( Image credit: Convolutional Sequence Generation for Skeleton-Based Action Synthesis )"
        ],
        "original_definition_source": [
            "http://www.dahualin.org/publications/dhl19_csgn.pdf"
        ],
        "subtasks": [
            "Action Generation"
        ]
    },
    "image-manipulation-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-morphing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-quality-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-stitching": {
        "definition": "",
        "original_definition": [
            "Image Stitching is a process of composing multiple images with narrow but overlapping fields of view to create a larger image with a wider field of view.",
            "Source: Single-Perspective Warps in Natural Image Stitching",
            "( Image credit: Kornia )"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1802.04645"
        ],
        "subtasks": []
    },
    "license-plate-detection": {
        "definition": "",
        "original_definition": [
            "License Plate Recognition is an image-processing technology used to identify vehicles by their license plates. This technology is used in various security and traffic applications."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "motion-detection": {
        "definition": "",
        "original_definition": [
            "Motion Detection is a process to detect the presence of any moving entity in an area of interest. Motion Detection is of great importance due to its application in various areas such as surveillance and security, smart homes, and health monitoring.",
            "Source: Different Approaches for Human Activity Recognition\u2013 A Survey"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1906.05074"
        ],
        "subtasks": []
    },
    "image-smoothing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "facial-expression-recognition": {
        "definition": "",
        "original_definition": [
            "Facial expression recognition is the task of classifying the expressions on face images into various categories such as anger, fear, surprise, sadness, happiness and so on.",
            "( Image credit: DeXpression )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1509.05371v2.pdf"
        ],
        "subtasks": [
            "Micro-Expression Recognition",
            "Micro-Expression Spotting",
            "3D Facial Expression Recognition",
            "Smile Recognition"
        ]
    },
    "image-deconvolution": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "material-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "occlusion-handling": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "person-identification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unsupervised-object-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Self-Organized Clustering"
        ]
    },
    "text-to-image-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Zero-Shot Text-to-Image Generation",
            "text-guided-image-editing"
        ]
    },
    "contour-detection": {
        "definition": "",
        "original_definition": [
            "Object Contour Detection extracts information about the object shape in images.",
            "Source: Object Contour and Edge Detection with RefineContourNet"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1904.13353"
        ],
        "subtasks": []
    },
    "highlight-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-object-tracking-and-segmentation": {
        "definition": "",
        "original_definition": [
            "Multiple object tracking and segmentation requires detecting, tracking, and segmenting objects belonging to a set of given classes.",
            "(Image and definition credit: Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation, NeurIPS 2021, Spotlight )"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-target-domain-adaptation": {
        "definition": "",
        "original_definition": [
            "The idea of Multi-target Domain Adaptation is to adapt a model from a single labelled source domain to multiple unlabelled target domains."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "road-damage-detection": {
        "definition": "",
        "original_definition": [
            "Road damage detection is the task of detecting damage in roads.",
            "( Image credit: Road Damage Detection And Classification In Smartphone Captured Images Using Mask R-CNN )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1811.04535v1.pdf"
        ],
        "subtasks": []
    },
    "sar-image-despeckling": {
        "definition": "",
        "original_definition": [
            "Despeckling is the task of suppressing speckle from Synthetic Aperture Radar (SAR) acquisitions.",
            "Image credits: GRD Sentinel-1 SAR image despeckled with SAR2SAR-GRD"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "sign-language-translation": {
        "definition": "",
        "original_definition": [
            "Given a video containing sign language, the task is to predict the translation into (written) spoken language.",
            "Image credit: How2Sign"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "symmetry-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "table-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "text-based-person-retrieval": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "weakly-supervised-instance-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "severity-prediction": {
        "definition": "",
        "original_definition": [
            "This task aims to solve absolute 3D multi-person pose Estimation (camera-centric coordinates). No ground truth human bounding box and human root joint coordinates are used during testing stage.",
            "( Image credit: RootNet )"
        ],
        "original_definition_source": [
            "https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE"
        ],
        "subtasks": []
    },
    "affordance-detection": {
        "definition": "",
        "original_definition": [
            "Affordance detection refers to identifying the potential action possibilities of objects in an image, which is an important ability for robot perception and manipulation.",
            "Image source: Object-Based Affordances Detection with Convolutional Neural Networks and Dense Conditional Random Fields",
            "Unlike other visual or physical properties that mainly describe the object alone, affordances indicate functional interactions of object parts with humans."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "art-analysis": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "face-reenactment": {
        "definition": "",
        "original_definition": [
            "Face Reenactment is an emerging conditional face synthesis task that aims at fulfilling two goals simultaneously: 1) transfer a source face shape to a target face; while 2) preserve the appearance and the identity of the target face.",
            "Source: One-shot Face Reenactment"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1908.03251"
        ],
        "subtasks": []
    },
    "hand-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-mesh-recovery": {
        "definition": "",
        "original_definition": [
            "Estimate 3D body mesh from images"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-instance-retrieval": {
        "definition": "",
        "original_definition": [
            "Image Instance Retrieval is the problem of retrieving images from a database representing the same object or scene as the one depicted in a query image.",
            "Source: Compression of Deep Neural Networks for Image Instance Retrieval"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1701.04923"
        ],
        "subtasks": [
            "Amodal Instance Segmentation"
        ]
    },
    "person-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "person-retrieval": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "traffic-sign-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-understanding": {
        "definition": "",
        "original_definition": [
            "A crucial task of Video Understanding is to recognise and localise (in space and time) different actions or events appearing in the video.",
            "Source: Action Detection from a Robot-Car Perspective"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1807.11332"
        ],
        "subtasks": [
            "Video Alignment"
        ]
    },
    "text-to-image": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Story Visualization",
            "VGSI"
        ]
    },
    "fake-image-detection": {
        "definition": "",
        "original_definition": [
            "This task aims to solve root-relative 3D multi-person pose estimation (person-centric coordinate system). No ground truth human bounding box and human root joint coordinates are used during testing stage.",
            "( Image credit: RootNet )"
        ],
        "original_definition_source": [
            "https://github.com/mks0601/3DMPPE_ROOTNET_RELEASE"
        ],
        "subtasks": []
    },
    "action-analysis": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "crop-yield-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "facial-editing": {
        "definition": "",
        "original_definition": [
            "Image source: Stitch it in Time: GAN-Based Facial Editing of Real Videos"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "multiview-learning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "skills-assessment": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "space-time-video-super-resolution": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "sports-analytics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "universal-domain-adaptation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-hand-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Image: Zimmerman et l"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Grasp Generation",
            "3D Canonical Hand Pose Estimation"
        ]
    },
    "face-recognition": {
        "definition": "",
        "original_definition": [
            "Facial recognition is the task of making a positive identification of a face in a photo or video image against a pre-existing database of faces. It begins with detection - distinguishing human faces from other objects in the image - and then works on identification of those detected faces.",
            "The state of the art tables for this task are contained mainly in the consistent parts of the task : the face verification and face identification tasks.",
            "( Image credit: Face Verification )"
        ],
        "original_definition_source": [
            "https://shuftipro.com/face-verification"
        ],
        "subtasks": [
            "Age-Invariant Face Recognition",
            "Face Quality Assessement"
        ]
    },
    "instance-search": {
        "definition": "",
        "original_definition": [
            "Visual Instance Search is the task of retrieving from a database of images the ones that contain an instance of a visual query. It is typically much more challenging than finding images from the database that contain objects belonging to the same category as the object in the query. If the visual query is an image of a shoe, visual Instance Search does not try to find images of shoes, which might differ from the query in shape, color or size, but tries to find images of the exact same shoe as the one in the query image. Visual Instance Search challenges image representations as the features extracted from the images must enable such fine-grained recognition despite variations in viewpoints, scale, position, illumination, etc. Whereas holistic image representations, where each image is mapped to a single high-dimensional vector, are sufficient for coarse-grained similarity retrieval, local features are needed for instance retrieval.",
            "Source: Dynamicity and Durability in Scalable Visual Instance Search"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1805.10942"
        ],
        "subtasks": [
            "Audio Fingerprint"
        ]
    },
    "3d-point-cloud-linear-classification": {
        "definition": "",
        "original_definition": [
            "Training a linear classifier(e.g. SVM) on the embeddings/representations of 3D point clouds. The embeddings/representations are usually trained in an unsupervised manner."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-shape-modeling": {
        "definition": "",
        "original_definition": [
            "Image: Gkioxari et al"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "foveation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-steganography": {
        "definition": "",
        "original_definition": [
            "Image Steganography is the main content of information hiding. The sender conceal a secret message into a cover image, then get the container image called stego, and finish the secret message\u2019s transmission on the public channel by transferring the stego image. Then the receiver part of the transmission can reveal the secret message out. Steganalysis is an attack to the steganography algorithm. The listener on the public channel intercept the image and analyze whether the image contains secret information.",
            "Source: Invisible Steganography via Generative Adversarial Networks"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1807.08571"
        ],
        "subtasks": []
    },
    "material-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "point-cloud-super-resolution": {
        "definition": "",
        "original_definition": [
            "Point cloud super-resolution is a fundamental problem for 3D reconstruction and 3D data understanding. It takes a low-resolution (LR) point cloud as input and generates a high-resolution (HR) point cloud with rich details"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "scene-graph-generation": {
        "definition": "",
        "original_definition": [
            "A scene graph is a structured representation of an image, where nodes in a scene graph correspond to object bounding boxes with their object categories, and edges correspond to their pairwise relationships between objects. The task of Scene Graph Generation is to generate a visually-grounded scene graph that most accurately correlates with an image.",
            "Source: Scene Graph Generation by Iterative Message Passing"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1701.02426"
        ],
        "subtasks": [
            "Unbiased Scene Graph Generation"
        ]
    },
    "single-object-discovery": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "skills-evaluation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unsupervised-image-to-image-translation": {
        "definition": "",
        "original_definition": [
            "Unsupervised image-to-image translation is the task of doing image-to-image translation without ground truth image-to-image pairings.",
            "( Image credit: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1703.10593v6.pdf"
        ],
        "subtasks": [
            "Sensor Modeling"
        ]
    },
    "autonomous-navigation": {
        "definition": "",
        "original_definition": [
            "Autonomous navigation is the task of autonomously navigating a vehicle or robot to or around a location without human guidance.",
            "( Image credit: Approximate LSTMs for Time-Constrained Inference: Enabling Fast Reaction in Self-Driving Cars )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1905.00689v2.pdf"
        ],
        "subtasks": [
            "Sequential Place Recognition",
            "Autonomous Flight (Dense Forest)"
        ]
    },
    "multimodal-machine-translation": {
        "definition": "",
        "original_definition": [
            "Multimodal machine translation is the task of doing machine translation with multiple data sources - for example, translating \"a bird is flying over water\" + an image of a bird over water to German text.",
            "( Image credit: Findings of the Third Shared Task on Multimodal Machine Translation )"
        ],
        "original_definition_source": [
            "https://www.aclweb.org/anthology/W18-6402.pdf"
        ],
        "subtasks": [
            "Face to Face Translation",
            "Multimodal Lexical Translation"
        ]
    },
    "birds-eye-view-object-detection": {
        "definition": "",
        "original_definition": [
            "KITTI birds eye view detection task"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "gan-image-forensics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "handwriting-generation": {
        "definition": "",
        "original_definition": [
            "The inverse of handwriting recognition. From text generate and image of handwriting (offline) of trajectory of handwriting (online)."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "horizon-line-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "long-tailed-learning": {
        "definition": "",
        "original_definition": [
            "Long-tailed Learning"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "referring-expression-generation": {
        "definition": "",
        "original_definition": [
            "Generate referring expressions"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "segmentation-of-remote-sensing-imagery": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Lake Ice Monitoring"
        ]
    },
    "talking-face-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Lake Ice Monitoring"
        ]
    },
    "unsupervised-3d-point-cloud-linear-evaluation": {
        "definition": "",
        "original_definition": [
            "Training a linear classifier(e.g. SVM) on the representations learned in an unsupervised manner on the pretrained(e.g. ShapeNet) dataset."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-style-transfer": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "zero-shot-transfer-image-classification": {
        "definition": "",
        "original_definition": [
            "Image: Liao et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Shape from Texture"
        ]
    },
    "video-prediction": {
        "definition": "",
        "original_definition": [
            "Video Prediction is the task of predicting future frames given past video frames.",
            "Source: Photo-Realistic Video Prediction on Natural Videos of Largely Changing Frames"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2003.08635"
        ],
        "subtasks": [
            "Predict Future Video Frames",
            "Earth Surface Forecasting"
        ]
    },
    "3d-semantic-scene-completion": {
        "definition": "",
        "original_definition": [
            "This task was introduced in \"Semantic Scene Completion from a Single Depth Image\" (https://arxiv.org/abs/1611.08974) at CVPR 2017 . The target is to infer the dense 3D voxelized semantic scene from an incompleted 3D input (e.g. point cloud, depth map) and an optional RGB image. A recent summary can be found in the paper \"3D Semantic Scene Completion: a Survey\" (https://arxiv.org/abs/2103.07466), published at IJCV 2021."
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Semantic Scene Completion from a single RGB image"
        ]
    },
    "3d-shape-representation": {
        "definition": "",
        "original_definition": [
            "Image: MeshNet"
        ],
        "original_definition_source": [],
        "subtasks": [
            "3D Dense Shape Correspondence"
        ]
    },
    "affordance-recognition": {
        "definition": "",
        "original_definition": [
            "Affordance recognition from Human-Object Interaction"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-deblocking": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-imputation": {
        "definition": "",
        "original_definition": [
            "Image imputation is the task of creating plausible images from low-resolution images or images with missing data.",
            "( Image credit: NASA )"
        ],
        "original_definition_source": [
            "https://www.jpl.nasa.gov/edu/news/2019/4/19/how-scientists-captured-the-first-image-of-a-black-hole/"
        ],
        "subtasks": []
    },
    "image-similarity-search": {
        "definition": "",
        "original_definition": [
            "Image credit: The 2021 Image Similarity Dataset and Challenge"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "medical-image-denoising": {
        "definition": "",
        "original_definition": [
            "Image credit: Learning Medical Image Denoising with Deep Dynamic Residual Attention Network"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "multispectral-object-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "pose-retrieval": {
        "definition": "",
        "original_definition": [
            "Retrieval of similar human poses from images or videos"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "scanpath-prediction": {
        "definition": "",
        "original_definition": [
            "Learning to Predict Sequences of Human Fixations."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "scene-change-detection": {
        "definition": "",
        "original_definition": [
            "Scene change detection (SCD) refers to the task of localizing changes and identifying change-categories given two scenes. A scene can be either an RGB (+D) image or a 3D reconstruction (point cloud). If the scene is an image, SCD is a form of pixel-level prediction because each pixel in the image is classified according to a category. On the other hand, if the scene is point cloud, SCD is a form of point-level prediction because each point in the cloud is classified according to a category.",
            "Some example benchmarks for this task are VL-CMU-CD, PCD, and CD2014. Recently, more complicated benchmarks such as ChangeSim, HDMap, and Mallscape are released.",
            "Models are usually evaluated with the Mean Intersection-Over-Union (Mean IoU), Pixel Accuracy, or F1 metrics."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "single-view-3d-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "3D Semantic Scene Completion from a single RGB image"
        ]
    },
    "video-matting": {
        "definition": "",
        "original_definition": [
            "Image credit: https://arxiv.org/pdf/2012.07810v1.pdf"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Camera shot boundary detection"
        ]
    },
    "zero-shot-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "inverse-tone-mapping": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "deception-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Deception Detection In Videos"
        ]
    },
    "landmark-tracking": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Muscle Tendon Junction Identification"
        ]
    },
    "semi-supervised-anomaly-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Physical Video Anomaly Detection",
            "General Action Video Anomaly Detection"
        ]
    },
    "6d-pose-estimation": {
        "definition": "",
        "original_definition": [
            "6D pose estimation is the task of detecting the 6D pose of an object, which include its location and orientation. This is an important task in robotics, where a robotic arm needs to know the location and orientation to detect and move objects in its vicinity successfully. This allows the robot to operate safely and effectively alongside humans. The awareness of the position and orientation of objects in a scene is sometimes referred to as 6D, where the D stands for degrees of freedom pose.",
            "( Image credit: Segmentation-driven 6D Object Pose Estimation )"
        ],
        "original_definition_source": [
            "https://github.com/cvlab-epfl/segmentation-driven-pose"
        ],
        "subtasks": []
    },
    "ad-hoc-video-search": {
        "definition": "",
        "original_definition": [
            "The Ad-hoc search task ended a 3 year cycle from 2016-2018 with a goal to model the end user search use-case, who is searching (using textual sentence queries) for segments of video containing persons, objects, activities, locations, etc. and combinations of the former. While the Internet Archive (IACC.3) dataset was adopted between 2016 to 2018, starting in 2019 a new data collection based on Vimeo Creative Commons (V3C) will be adopted to support the task for at least 3 more years.",
            "Given the test collection (V3C1 or IACC.3), master shot boundary reference, and set of Ad-hoc queries (approx. 30 queries) released by NIST, return for each query a list of at most 1000 shot IDs from the test collection ranked according to their likelihood of containing the target query."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "handwriting-verification": {
        "definition": "",
        "original_definition": [
            "The goal of handwriting verification is to find a measure of confidence whether the given handwritten samples are written by the same or different writer."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-shadow-removal": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "instance-shadow-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Shadow Detection And Removal"
        ]
    },
    "jpeg-artifact-removal": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "multiple-people-tracking": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "occlusion-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "semi-supervised-instance-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "spatial-relation-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "traffic-accident-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Accident Anticipation"
        ]
    },
    "video-forensics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "visual-grounding": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Person-centric Visual Grounding"
        ]
    },
    "motion-retargeting": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "referring-image-matting": {
        "definition": "",
        "original_definition": [
            "Extracting the meticulous alpha matte of the specific object from the image that can best match the given natural language description"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Referring Image Matting (Prompt-based)",
            "Referring Image Matting (Expression-based)"
        ]
    },
    "3d-depth-estimation": {
        "definition": "",
        "original_definition": [
            "Image: monodepth2"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Transparent Object Depth Estimation"
        ]
    },
    "3d-rotation-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "action-assessment": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "camera-auto-calibration": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "defocus-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "face-detection": {
        "definition": "",
        "original_definition": [
            "Face detection is the task of detecting faces in a photo or video (and distinguishing them from other objects).",
            "( Image credit: insightface )"
        ],
        "original_definition_source": [
            "https://github.com/deepinsight/insightface"
        ],
        "subtasks": [
            "Occluded Face Detection"
        ]
    },
    "fingertip-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "font-recognition": {
        "definition": "",
        "original_definition": [
            "Font recognition (also called visual font recognition or optical font recognition) is the task of identifying the font family or families used in images containing text. Understanding which fonts are used in text may, for example, help designers find the right style, as well as help select an optical character recognition engine or model that is a better fit for certain texts."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-object-interaction-concept-discovery": {
        "definition": "",
        "original_definition": [
            "Discovering the reasonable HOI concepts/categories from known categories and their instances. Actually, it is also a matrix (verb-object matrix) complementation problem."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "lane-detection": {
        "definition": "",
        "original_definition": [
            "Lane detection is the task of detecting lanes on a road from a camera.",
            "( Image credit: End-to-end Lane Detection )"
        ],
        "original_definition_source": [
            "https://github.com/wvangansbeke/LaneDetection_End2End"
        ],
        "subtasks": [
            "3D Lane Detection"
        ]
    },
    "lip-to-speech-synthesis": {
        "definition": "",
        "original_definition": [
            "Given a silent video of a speaker, generate the corresponding speech that matches the lip movements."
        ],
        "original_definition_source": [],
        "subtasks": [
            "Speaker-Specific Lip to Speech Synthesis"
        ]
    },
    "personality-trait-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "physiological-computing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "pornography-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "scene-aware-dialogue": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "semi-supervised-video-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "spatio-temporal-video-grounding": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "text-to-face-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unsupervised-image-decomposition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unsupervised-landmark-detection": {
        "definition": "",
        "original_definition": [
            "The discovery of object landmarks on a set of images depicting objects of the same category, directly from raw images without using any manual annotations."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "vehicle-speed-estimation": {
        "definition": "",
        "original_definition": [
            "Vehicle speed estimation is the task of detecting and tracking vehicles whose real-world speeds are then estimated. The task is usually evaluated with recall and precision of the detected vehicle tracks as well as the mean or median errors of the estimated vehicle speeds."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-propagation": {
        "definition": "",
        "original_definition": [
            "Propagating information in processed frames to unprocessed frames"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "gaze-redirection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-denoising": {
        "definition": "",
        "original_definition": [
            "Image Denoising is the task of removing noise from an image, e.g. the application of Gaussian noise to an image.",
            "( Image credit: Wide Inference Network for Image Denoising via Learning Pixel-distribution Prior )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1707.05414v5.pdf"
        ],
        "subtasks": [
            "lifetime image denoising",
            "intensity image denoising"
        ]
    },
    "observation-completion": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Active Observation Completion"
        ]
    },
    "road-scene-understanding": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Monocular Cross-View Road Scene Parsing(Road)",
            "Monocular Cross-View Road Scene Parsing(Vehicle)"
        ]
    },
    "4d-spatio-temporal-semantic-segmentation": {
        "definition": "",
        "original_definition": [
            "Image: Choy et al"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "action-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Video Generation from a Single Image"
        ]
    },
    "action-segmentation": {
        "definition": "",
        "original_definition": [
            "Action Segmentation is a challenging problem in high-level video understanding. In its simplest form, Action Segmentation aims to segment a temporally untrimmed video by time and label each segmented part with one of pre-defined action labels. The results of Action Segmentation can be further used as input to various applications, such as video-to-text and action localization.",
            "Source: TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1705.07818"
        ],
        "subtasks": [
            "Weakly Supervised Action Segmentation (Transcript)"
        ]
    },
    "animated-gif-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "audio-visual-synchronization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "brdf-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "bokeh-effect-rendering": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "camouflage-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "data-ablation": {
        "definition": "",
        "original_definition": [
            "Data Ablation is the study of change in data, and its effects in the performance of Neural Networks."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "depth-image-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "detect-forged-images-and-videos": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "dynamic-texture-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "event-data-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "face-anonymization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "face-verification": {
        "definition": "",
        "original_definition": [
            "Face verification is the task of comparing a candidate face to another, and verifying whether it is a match. It is a one-to-one mapping: you have to check if this person is the correct one.",
            "( Image credit: Pose-Robust Face Recognition via Deep Residual Equivariant Mapping )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1803.00839v1.pdf"
        ],
        "subtasks": [
            "Disguised Face Verification"
        ]
    },
    "food-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "gait-identification": {
        "definition": "",
        "original_definition": [
            "The average of the normalized top-1 prediction scores of unseen classes in the generalized zero-shot learning setting, where the label of a test sample is predicted among all (seen + unseen) classes."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "historical-color-image-dating": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "materials-imaging": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-person-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Multi-person pose estimation is the task of estimating the pose of multiple people in one frame.",
            "( Image credit: Human Pose Estimation with TensorFlow )"
        ],
        "original_definition_source": [
            "https://github.com/eldar/pose-tensorflow"
        ],
        "subtasks": [
            "Semi-Supervised Human Pose Estimation"
        ]
    },
    "multi-modal-image-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-object-discovery": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "neural-radiance-caching": {
        "definition": "",
        "original_definition": [
            "Involves the task of predicting photorealistic pixel colors from feature buffers.",
            "Image source: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "neural-rendering": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "patch-matching": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Multimodal Patch Matching"
        ]
    },
    "svbrdf-estimation": {
        "definition": "",
        "original_definition": [
            "SVBRDF Estimation"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "sketch-based-image-retrieval": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "On-the-Fly Sketch Based Image Retrieval"
        ]
    },
    "spectrum-cartography": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "trademark-retrieval": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "vehicle-key-point-and-orientation-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-generation": {
        "definition": "",
        "original_definition": [
            "( Image credit: Logacheva et al. )"
        ],
        "original_definition_source": [
            "https://paperswithcode.com/paper/deeplandscape-adversarial-modeling-of-1"
        ],
        "subtasks": [
            "Video Generation from a Single Image"
        ]
    },
    "visual-relationship-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Video Visual Relation Detection"
        ]
    },
    "visual-sentiment-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "weakly-supervised-panoptic-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "drone-based-object-tracking": {
        "definition": "",
        "original_definition": [
            "drone-based object tracking"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "fashion-understanding": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Semi-Supervised Fashion Compatibility"
        ]
    },
    "forgery": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Localization In Video Forgery"
        ]
    },
    "3d-canonicalization": {
        "definition": "",
        "original_definition": [
            "3D Canonicalization is the process of estimating a transformation-invariant feature for classification and part segmentation tasks."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-object-classification": {
        "definition": "",
        "original_definition": [
            "Image: Sedaghat et al"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Cube Engraving Classification"
        ]
    },
    "3d-surface-generation": {
        "definition": "",
        "original_definition": [
            "Image: AtlasNet"
        ],
        "original_definition_source": [],
        "subtasks": [
            "Visibility Estimation from Point Cloud"
        ]
    },
    "amodal-layout-estimation": {
        "definition": "",
        "original_definition": [
            "Amodal scene layout estimation involves estimating the static and dynamic portion of an urban driving scene in bird's-eye view, given a single image. The concept of \"amodal\" estimation refers to the fact that we also estimate layout of parts of the scene that are not observable in the image."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "blink-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "constrained-diffeomorphic-image-registration": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "detecting-shadows": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "drivable-area-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "fashion-compatibility-learning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "few-shot-image-classification": {
        "definition": "",
        "original_definition": [
            "Few-shot image classification is the task of doing image classification with only a few examples for each category (typically < 6 examples).",
            "( Image credit: Learning Embedding Adaptation for Few-Shot Learning )"
        ],
        "original_definition_source": [
            "https://github.com/Sha-Lab/FEAT"
        ],
        "subtasks": [
            "Generalized Few-Shot Classification"
        ]
    },
    "fine-grained-image-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "house-generation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-fmri-response-prediction": {
        "definition": "",
        "original_definition": [
            "The task is: Given a) the set of videos of everyday events and b) the corresponding brain responses recorded while human participants viewed those videos, use computational models to predict brain responses for videos."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "hurricane-forecasting": {
        "definition": "",
        "original_definition": [
            "Tropical Cyclone Forecasting using Computer Vision, Deep Learning, and Time-Series methods"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "ifc-entity-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "im2spec": {
        "definition": "",
        "original_definition": [
            "Predicting spectra from images (and vice versa)"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-declipping": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-similarity-detection": {
        "definition": "",
        "original_definition": [
            "A fundamental computer vision task to determine whether a part of an image has been copied from another image.",
            "Description from: The 2021 Image Similarity Dataset and Challenge",
            "Image credit: The 2021 Image Similarity Dataset and Challenge"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-stylization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-to-gps-verification": {
        "definition": "",
        "original_definition": [
            "The image-to-GPS verification task asks whether a given image is taken at a claimed GPS location.",
            "( Image credit: Image-to-GPS Verification Through A Bottom-Up Pattern Matching Network )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1811.07288v1.pdf"
        ],
        "subtasks": []
    },
    "image-based-automatic-meter-reading": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Dial Meter Reading"
        ]
    },
    "indoor-scene-reconstruction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Plan2Scene"
        ]
    },
    "infrared-image-super-resolution": {
        "definition": "",
        "original_definition": [
            "Aims at upsampling the IR image and create the high resolution image with help of a low resolution image."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "jpeg-decompression": {
        "definition": "",
        "original_definition": [
            "Image credit: Palette: Image-to-Image Diffusion Models"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "kiss-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "landmark-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Brain landmark detection"
        ]
    },
    "landmine": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "language-based-temporal-localization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Corpus Video Moment Retrieval"
        ]
    },
    "lightfield": {
        "definition": "",
        "original_definition": [
            "Tasks related to the light-field imagery"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "manufacturing-quality-control": {
        "definition": "",
        "original_definition": [
            "AI for Quality control in manufacturing processes."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "mental-workload-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "metamerism": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "meter-reading": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Image-based Automatic Meter Reading"
        ]
    },
    "mistake-detection": {
        "definition": "",
        "original_definition": [
            "Mistakes are natural occurrences in many tasks and an opportunity for an AR assistant to provide help. Identifying such mistakes requires modelling procedural knowledge and retaining long-range sequence information. In its simplest form Mistake Detection aims to classify each coarse action segment into one of the three classes: {\u201ccorrect\u201d, \u201cmistake\u201d, \u201ccorrection\u201d}."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "multi-oriented-scene-text-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Natural Image Orientation Angle Detection"
        ]
    },
    "multi-person-pose-estimation-and-tracking": {
        "definition": "",
        "original_definition": [
            "Joint multi-person pose estimation and tracking following the PoseTrack benchmark. https://posetrack.net/",
            "( Image credit: PoseTrack )"
        ],
        "original_definition_source": [
            "https://github.com/iqbalu/PoseTrack-CVPR2017"
        ],
        "subtasks": []
    },
    "multi-object-colocalization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "multiple-object-tracking": {
        "definition": "",
        "original_definition": [
            "Multiple Object Tracking is the problem of automatically identifying multiple objects in a video and representing them as a set of trajectories with high accuracy.",
            "Source: SOT for MOT"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/1712.01059"
        ],
        "subtasks": [
            "Multiple Object Track and Segmentation"
        ]
    },
    "neural-stylization": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "occluded-3d-object-symmetry-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "open-set-video-captioning": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "pso-convnets-dynamics-1": {
        "definition": "",
        "original_definition": [
            "Incorporating distilled Cucker-Smale elements into PSO algorithm using KNN and intertwine training with SGD"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "pso-convnets-dynamics-2": {
        "definition": "",
        "original_definition": [
            "Incorporating distilled Cucker-Smale elements into PSO algorithm using KNN and intertwine training with SGD (Pull back method)"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "parking-space-occupancy": {
        "definition": "",
        "original_definition": [
            "Image credit: https://github.com/martin-marek/parking-space-occupancy"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "partial-point-cloud-matching": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "partially-view-aligned-multi-view-learning": {
        "definition": "",
        "original_definition": [
            "In multi-view learning, Partially View-aligned Problem (PVP) refers to the case when only a portion of data is aligned, thus leading to data inconsistency."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "pedestrian-detection": {
        "definition": "",
        "original_definition": [
            "Pedestrian detection is the task of detecting pedestrians from a camera.",
            "Further state-of-the-art results (e.g. on the KITTI dataset) can be found at 3D Object Detection.",
            "( Image credit: High-level Semantic Feature Detection: A New Perspective for Pedestrian Detection )"
        ],
        "original_definition_source": [
            "https://github.com/liuwei16/CSP"
        ],
        "subtasks": [
            "Thermal Infrared Pedestrian Detection"
        ]
    },
    "physical-attribute-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "point-cloud-classification-dataset": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "population-mapping": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "prostate-zones-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "pulmorary-vessel-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Pulmonary Artery\u2013Vein Classification"
        ]
    },
    "safety-perception-recognition": {
        "definition": "",
        "original_definition": [
            "City safety perception recognition"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "steganographics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "surface-normals-estimation-from-point-clouds": {
        "definition": "",
        "original_definition": [
            "Parent task: 3d Point Clouds Analysis"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "thermal-image-denoising": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "transform-a-video-into-a-comics": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "transparency-separation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "typeface-completion": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unbalanced-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "uncropping": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "unsupervised-semantic-segmentation": {
        "definition": "",
        "original_definition": [
            "Models that learn to segment each image (i.e. cluster the pixels into their ground truth classes) without seeing the ground truth labels.",
            "( Image credit: SegSort: Segmentation by Discriminative Sorting of Segments )"
        ],
        "original_definition_source": [
            "http://openaccess.thecvf.com/content_ICCV_2019/papers/Hwang_SegSort_Segmentation_by_Discriminative_Sorting_of_Segments_ICCV_2019_paper.pdf"
        ],
        "subtasks": [
            "Unsupervised Semantic Segmentation with Language-image Pre-training"
        ]
    },
    "video-correspondence-flow": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-frame-interpolation": {
        "definition": "",
        "original_definition": [
            "The goal of Video Frame Interpolation is to synthesize several frames in the middle of two adjacent frames of the original video. Video Frame Interpolation can be applied to generate slow motion video, increase video frame rate, and frame recovery in video streaming.",
            "Source: Reducing the X-ray radiation exposure frequency in cardio-angiography via deep-learning based video interpolation"
        ],
        "original_definition_source": [
            "https://arxiv.org/abs/2006.00781"
        ],
        "subtasks": [
            "eXtreme-Video-Frame-Interpolation"
        ]
    },
    "video-individual-counting": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "visual-social-relationship-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "visual-speech-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Lip to Speech Synthesis"
        ]
    },
    "weakly-supervised-3d-point-cloud-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "window-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "yield-mapping-in-apple-orchards": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "extreme-video-frame-interpolation": {
        "definition": "",
        "original_definition": [
            "Type of Video Frame Interpolation (VFI) that interpolates an intermediate frame on X4K1000FPS dataset containing 4K videos of 1000 fps with the extreme motion. The dataset has a wide variety of textures, extremely large motions, zoomings and occlusions, which have never been seen in the previous VFI benchmark datasets."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "human-motion-similarity": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "interestingness-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "micro-doppler-signatures-discrimination": {
        "definition": "",
        "original_definition": [
            "Classification, anomaly detection and clustering to separate radar targets using micro-Doppler signatures."
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "motion-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "OPD: Single-view 3D Openable Part Detection"
        ]
    },
    "spectral-estimation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "Spectral Estimation From A Single Rgb Image"
        ]
    },
    "computational-manga": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": [
            "panel extraction"
        ]
    },
    "3d-canonical-hand-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Image: Lin et al"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-prostate-segmentation": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "comics-processing": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "damaged-building-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "document-to-image-conversion": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "dynamics-1": {
        "definition": "",
        "original_definition": [
            "Incorporating distilled Cucker-Smale elements into PSO algorithm using KNN and intertwine training with SGD"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "earthquake-prediction": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "frame-duplication-detection": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "image-comprehension": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "kinematic-based-workflow-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "logo-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "motion-detection-in-non-stationary-scenes": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "prediction-of-occupancy-grid-maps": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "segmentation-based-workflow-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "small-object-detection": {
        "definition": "",
        "original_definition": [
            "Small object detection is the task of detecting small objects.",
            "( Image credit: Feature-Fused SSD )"
        ],
        "original_definition_source": [
            "https://arxiv.org/pdf/1709.05054v3.pdf"
        ],
        "subtasks": [
            "Rice Grain Disease Detection"
        ]
    },
    "sperm-morphology-classification": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "video-based-workflow-recognition": {
        "definition": "",
        "original_definition": [],
        "original_definition_source": [],
        "subtasks": []
    },
    "3d-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Image credit: GSNet: Joint Vehicle Pose and Shape Reconstruction with Geometrical and Scene-aware Supervision , ECCV'20"
        ],
        "original_definition_source": [],
        "subtasks": []
    },
    "animal-pose-estimation": {
        "definition": "",
        "original_definition": [
            "Animal pose estimation is the task of identifying the pose of an animal.",
            "( Image credit: Using DeepLabCut for 3D markerless pose estimation across species and behaviors )"
        ],
        "original_definition_source": [
            "http://www.mousemotorlab.org/s/NathMathis2019.pdf"
        ],
        "subtasks": []
    }
}